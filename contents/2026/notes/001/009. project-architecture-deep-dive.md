# Project Architecture Deep Dive - Two Selected Projects

## Project 1: Conversational AI Platform (LivePerson/Digit88)
**Scale**: 12M+ conversations/month | **Team**: Principal Engineer/Engineering Manager

## Project 2: Prime Broker System (IG Group)
**Scale**: 1M+ trades/day | **Team**: Technical Architect

---

# Project 1: Conversational AI Platform - Architecture Deep Dive

## Overview

**Business Context:**
- LivePerson's conversational AI platform enabling real-time customer-agent conversations
- Handles chat, messaging, and AI-powered conversations
- Multi-tenant SaaS platform serving enterprise clients
- Scale: 12M+ conversations/month (3x growth during my tenure)

**Key Requirements:**
- Real-time message delivery (< 100ms latency)
- High availability (99.9% uptime)
- Horizontal scalability
- Multi-tenant isolation
- Integration with multiple NLU providers (IBM Watson, Google Dialog Flow)
- Agent state management and routing

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                        Client Applications                       │
│              (Web, Mobile, API Clients, Chat Widgets)            │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│                      API Gateway Layer                          │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │   REST API   │  │  WebSocket   │  │  GraphQL API  │         │
│  │   Gateway    │  │   Gateway    │  │   Gateway     │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│                    Microservices Layer                           │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │  Agent Match │  │  Conversation │  │   Bot        │         │
│  │   Service    │  │   Service    │  │  Service     │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │  NLU Facade  │  │  Message     │  │  Session     │         │
│  │   Service    │  │  Service     │  │  Service     │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
└────────────────────────────┬────────────────────────────────────┘
                             │
                ┌────────────┼────────────┐
                ↓            ↓            ↓
        ┌─────────────┐ ┌──────────┐ ┌──────────┐
        │   Kafka     │ │  Redis   │ │  Postgres│
        │ Event Bus   │ │  Cache   │ │   DB     │
        └─────────────┘ └──────────┘ └──────────┘
                             │
                             ↓
        ┌─────────────────────────────────────┐
        │     External NLU Services           │
        │  ┌──────────┐  ┌──────────┐        │
        │  │  IBM     │  │  Google  │        │
        │  │  Watson  │  │ DialogFlow│        │
        │  └──────────┘  └──────────┘        │
        └─────────────────────────────────────┘
```

---

## Core Components Architecture

### 1. Agent Match Service

**Purpose:**
- Manage agent availability and state
- Route conversations to appropriate agents
- Track agent activity and session state
- Enable dynamic load balancing

**Architecture:**

```
┌─────────────────────────────────────────────────────────┐
│              Agent Match Service                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  Agent State │  │   Routing    │  │   Event      │  │
│  │  Manager     │  │   Engine     │  │  Generator   │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└────────────────────────────┬─────────────────────────────┘
                             │
                ┌────────────┼────────────┐
                ↓            ↓            ↓
        ┌─────────────┐ ┌──────────┐ ┌──────────┐
        │   Redis     │ │  Kafka   │ │ Postgres │
        │ (Agent State)│ │ (Events) │ │ (Config) │
        └─────────────┘ └──────────┘ └──────────┘
```

**Key Design Decisions:**

1. **Stateless Service Design:**
   ```java
   @Service
   public class AgentMatchService {
       private final RedisTemplate<String, AgentState> redisTemplate;
       private final KafkaTemplate<String, AgentEvent> kafkaTemplate;
       
       public Agent matchAgent(ConversationRequest request) {
           // Read agent state from Redis (not local state)
           List<Agent> availableAgents = getAvailableAgents(request);
           
           // Apply routing logic
           Agent selectedAgent = routingEngine.selectAgent(availableAgents, request);
           
           // Update state in Redis
           updateAgentState(selectedAgent, AgentStatus.BUSY);
           
           // Emit event to Kafka
           emitAgentMatchedEvent(selectedAgent, request);
           
           return selectedAgent;
       }
   }
   ```

2. **Event-Driven State Management:**
   - Agent state stored in Redis for fast access
   - All state changes emit events to Kafka
   - Event sourcing pattern for audit trail
   - Real-time state synchronization across instances

3. **Dynamic Load Balancing:**
   ```java
   public class AgentRoutingEngine {
       public Agent selectAgent(List<Agent> agents, ConversationRequest request) {
           // Weighted round-robin based on:
           // - Current load
           // - Skill matching
           // - Availability
           // - Historical performance
           
           return agents.stream()
               .filter(agent -> matchesSkills(agent, request))
               .min(Comparator
                   .comparing(Agent::getCurrentLoad)
                   .thenComparing(Agent::getAverageResponseTime))
               .orElseThrow(() -> new NoAvailableAgentException());
       }
   }
   ```

**Challenges & Solutions:**

**Challenge 1: Agent State Consistency**
- **Problem**: Multiple service instances updating agent state simultaneously
- **Solution**: 
  - Used Redis distributed locks for state updates
  - Implemented optimistic locking with version numbers
  - Event-driven updates ensure eventual consistency

**Challenge 2: Real-time State Synchronization**
- **Problem**: Agent state changes need to be reflected immediately across all instances
- **Solution**:
  - Redis pub/sub for real-time state updates
  - Kafka events for state change notifications
  - TTL-based state expiration for stale data cleanup

**Challenge 3: High Availability**
- **Problem**: Service failure shouldn't lose agent state
- **Solution**:
  - Redis cluster with replication
  - State persisted to Kafka for recovery
  - Health checks and automatic failover

---

### 2. NLU Facade Service

**Purpose:**
- Abstract multiple NLU providers (IBM Watson, Google Dialog Flow)
- Provide unified interface for NLU operations
- Handle provider-specific differences
- Implement fallback and retry logic

**Architecture:**

```
┌─────────────────────────────────────────────────────────┐
│              NLU Facade Service                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  Provider    │  │   Adapter    │  │   Circuit    │  │
│  │  Registry    │  │   Factory    │  │   Breaker    │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │   Retry      │  │   Fallback   │  │   Caching    │  │
│  │   Manager    │  │   Strategy   │  │   Layer      │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└────────────────────────────┬─────────────────────────────┘
                             │
                ┌────────────┼────────────┐
                ↓            ↓            ↓
        ┌─────────────┐ ┌──────────┐ ┌──────────┐
        │  IBM Watson│ │  Google  │ │  Redis   │
        │   Client   │ │ DialogFlow│ │  Cache   │
        └─────────────┘ └──────────┘ └──────────┘
```

**Key Design Decisions:**

1. **Adapter Pattern:**
   ```java
   public interface NLUProvider {
       NLUResponse processMessage(String message, String conversationId);
       boolean isAvailable();
   }
   
   @Component
   public class IBMWatsonAdapter implements NLUProvider {
       private final WatsonAssistantService watsonService;
       
       @Override
       public NLUResponse processMessage(String message, String conversationId) {
           // Convert to IBM Watson format
           WatsonRequest request = convertToWatsonFormat(message, conversationId);
           
           // Call IBM Watson
           WatsonResponse response = watsonService.message(request);
           
           // Convert to common format
           return convertToCommonFormat(response);
       }
   }
   
   @Component
   public class GoogleDialogFlowAdapter implements NLUProvider {
       // Similar implementation for Google Dialog Flow
   }
   ```

2. **Provider Selection Strategy:**
   ```java
   @Service
   public class NLUFacadeService {
       private final List<NLUProvider> providers;
       private final CircuitBreaker circuitBreaker;
       
       public NLUResponse processMessage(String message, String conversationId) {
           // Try primary provider
           NLUProvider primaryProvider = selectProvider(conversationId);
           
           try {
               if (circuitBreaker.isOpen(primaryProvider)) {
                   return fallbackToSecondary(primaryProvider, message, conversationId);
               }
               
               return primaryProvider.processMessage(message, conversationId);
               
           } catch (Exception e) {
               // Fallback to secondary provider
               return fallbackToSecondary(primaryProvider, message, conversationId);
           }
       }
       
       private NLUResponse fallbackToSecondary(NLUProvider failedProvider, 
                                                String message, 
                                                String conversationId) {
           List<NLUProvider> availableProviders = providers.stream()
               .filter(p -> p != failedProvider)
               .filter(NLUProvider::isAvailable)
               .collect(Collectors.toList());
           
           for (NLUProvider provider : availableProviders) {
               try {
                   return provider.processMessage(message, conversationId);
               } catch (Exception e) {
                   // Try next provider
                   continue;
               }
           }
           
           throw new AllNLUProvidersUnavailableException();
       }
   }
   ```

3. **Caching Strategy:**
   ```java
   @Service
   public class NLUCacheService {
       private final RedisTemplate<String, NLUResponse> redisTemplate;
       
       public NLUResponse getCachedResponse(String messageHash) {
           return redisTemplate.opsForValue().get("nlu:" + messageHash);
       }
       
       public void cacheResponse(String messageHash, NLUResponse response) {
           // Cache for 5 minutes
           redisTemplate.opsForValue().set(
               "nlu:" + messageHash, 
               response, 
               Duration.ofMinutes(5)
           );
       }
   }
   ```

**Challenges & Solutions:**

**Challenge 1: Provider-Specific Differences**
- **Problem**: Each NLU provider has different API format, response structure, and capabilities
- **Solution**:
  - Adapter pattern to abstract differences
  - Common response model for internal use
  - Provider-specific converters

**Challenge 2: Provider Reliability**
- **Problem**: External NLU providers can be slow or unavailable
- **Solution**:
  - Circuit breaker pattern (Resilience4j)
  - Retry with exponential backoff
  - Fallback to secondary providers
  - Timeout handling (2-second timeout)

**Challenge 3: Cost Optimization**
- **Problem**: NLU API calls are expensive
- **Solution**:
  - Response caching for similar messages
  - Message deduplication
  - Batch processing where possible
  - Provider selection based on cost/performance

**Challenge 4: Response Time Variability**
- **Problem**: Different providers have different response times
- **Solution**:
  - Async processing with CompletableFuture
  - Parallel calls to multiple providers (race condition)
  - Response time monitoring and provider selection based on performance

---

### 3. Event-Driven Architecture

**Kafka Event Bus:**

```
┌─────────────────────────────────────────────────────────┐
│                    Kafka Cluster                        │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  agent-      │  │ conversation-│  │  message-    │  │
│  │  events      │  │  events      │  │  events      │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  session-    │  │  bot-        │  │  analytics-   │  │
│  │  events      │  │  events      │  │  events      │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└─────────────────────────────────────────────────────────┘
```

**Event Types:**

1. **Agent Events:**
   ```java
   public class AgentMatchedEvent {
       private String agentId;
       private String conversationId;
       private Instant timestamp;
       private AgentState previousState;
       private AgentState newState;
   }
   
   public class AgentStateChangedEvent {
       private String agentId;
       private AgentStatus status;
       private Instant timestamp;
       private String reason;
   }
   ```

2. **Conversation Events:**
   ```java
   public class ConversationStartedEvent {
       private String conversationId;
       private String customerId;
       private String channel;
       private Instant timestamp;
   }
   
   public class ConversationEndedEvent {
       private String conversationId;
       private String reason;
       private Duration duration;
       private Instant timestamp;
   }
   ```

**Challenges & Solutions:**

**Challenge 1: Event Ordering**
- **Problem**: Events must be processed in order for state consistency
- **Solution**:
  - Partition key based on conversationId/agentId
  - Single partition per entity ensures ordering
  - Idempotent event handlers

**Challenge 2: Event Schema Evolution**
- **Problem**: Event schemas change over time
- **Solution**:
  - Schema registry (Confluent Schema Registry)
  - Versioned events
  - Backward compatibility
  - Event migration strategies

**Challenge 3: Event Processing Latency**
- **Problem**: High event volume causing processing delays
- **Solution**:
  - Kafka consumer groups for parallel processing
  - Batch processing
  - Async event handlers
  - Monitoring and alerting

---

## Scaling Strategy

### Horizontal Scaling

**Before Scaling:**
- 4M conversations/month
- 2 service instances
- Single database instance
- No caching layer

**After Scaling:**
- 12M conversations/month (3x)
- 10+ service instances (auto-scaling)
- Database read replicas
- Redis cluster for caching
- Kafka cluster for event streaming

**Scaling Implementation:**

1. **Stateless Services:**
   ```yaml
   # Kubernetes Deployment
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: agent-match-service
   spec:
     replicas: 5
     template:
       spec:
         containers:
         - name: agent-match
           image: agent-match:latest
           resources:
             requests:
               memory: "512Mi"
               cpu: "500m"
             limits:
               memory: "1Gi"
               cpu: "1000m"
   ```

2. **Auto-Scaling:**
   ```yaml
   apiVersion: autoscaling/v2
   kind: HorizontalPodAutoscaler
   metadata:
     name: agent-match-hpa
   spec:
     scaleTargetRef:
       apiVersion: apps/v1
       kind: Deployment
       name: agent-match-service
     minReplicas: 3
     maxReplicas: 20
     metrics:
     - type: Resource
       resource:
         name: cpu
         target:
           type: Utilization
           averageUtilization: 70
     - type: Resource
       resource:
         name: memory
         target:
           type: Utilization
           averageUtilization: 80
   ```

3. **Database Scaling:**
   - Read replicas for read-heavy operations
   - Connection pooling (HikariCP)
   - Query optimization and indexing
   - Database sharding by tenant (future)

---

## Performance Optimizations

### 1. Caching Strategy

**Multi-Level Caching:**
```
Application Cache (Caffeine) → Redis Cache → Database
```

```java
@Service
public class ConversationCacheService {
    private final Cache<String, Conversation> localCache;
    private final RedisTemplate<String, Conversation> redisTemplate;
    
    public Conversation getConversation(String conversationId) {
        // L1: Local cache
        Conversation conversation = localCache.getIfPresent(conversationId);
        if (conversation != null) {
            return conversation;
        }
        
        // L2: Redis cache
        conversation = redisTemplate.opsForValue().get("conv:" + conversationId);
        if (conversation != null) {
            localCache.put(conversationId, conversation);
            return conversation;
        }
        
        // L3: Database
        conversation = conversationRepository.findById(conversationId);
        if (conversation != null) {
            redisTemplate.opsForValue().set("conv:" + conversationId, conversation, Duration.ofMinutes(10));
            localCache.put(conversationId, conversation);
        }
        
        return conversation;
    }
}
```

### 2. Database Optimization

**Connection Pooling:**
```yaml
spring:
  datasource:
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
```

**Query Optimization:**
```java
// Before: N+1 query problem
List<Conversation> conversations = conversationRepository.findAll();
for (Conversation conv : conversations) {
    Agent agent = agentRepository.findById(conv.getAgentId()); // N queries
}

// After: Single query with JOIN
@Query("SELECT c FROM Conversation c JOIN FETCH c.agent WHERE c.status = :status")
List<Conversation> findActiveConversationsWithAgent(@Param("status") ConversationStatus status);
```

### 3. Async Processing

```java
@Service
public class MessageProcessingService {
    private final ExecutorService executorService;
    
    @Async
    public CompletableFuture<NLUResponse> processMessageAsync(String message) {
        return CompletableFuture.supplyAsync(() -> {
            return nluFacadeService.processMessage(message);
        }, executorService);
    }
}
```

---

## Monitoring & Observability

### Metrics Collection

**Key Metrics:**
- Request rate (RPS)
- Response time (P50, P95, P99)
- Error rate
- Agent utilization
- Conversation throughput
- NLU provider response times

**Implementation:**
```java
@Component
public class MetricsCollector {
    private final MeterRegistry meterRegistry;
    
    public void recordRequest(String service, String operation, Duration duration) {
        Timer.Sample sample = Timer.start(meterRegistry);
        sample.stop(Timer.builder("request.duration")
            .tag("service", service)
            .tag("operation", operation)
            .register(meterRegistry));
    }
    
    public void recordError(String service, String errorType) {
        Counter.builder("request.errors")
            .tag("service", service)
            .tag("error.type", errorType)
            .register(meterRegistry)
            .increment();
    }
}
```

### Distributed Tracing

**Implementation:**
```java
@RestController
public class ConversationController {
    private final Tracer tracer;
    
    @PostMapping("/conversations")
    public ResponseEntity<Conversation> createConversation(@RequestBody ConversationRequest request) {
        Span span = tracer.nextSpan().name("create-conversation").start();
        try (Tracer.SpanInScope ws = tracer.withSpanInScope(span)) {
            // Business logic
            Conversation conversation = conversationService.create(request);
            return ResponseEntity.ok(conversation);
        } finally {
            span.end();
        }
    }
}
```

---

## Challenges Faced & Solutions

### Challenge 1: Scaling from 4M to 12M Conversations/Month

**Problem:**
- System couldn't handle 3x traffic increase
- Database became bottleneck
- High latency during peak hours
- Service instances crashing under load

**Solution:**
1. **Horizontal Scaling:**
   - Increased service instances from 2 to 10+
   - Implemented auto-scaling based on CPU/memory
   - Load balancer with health checks

2. **Database Optimization:**
   - Added read replicas
   - Implemented connection pooling
   - Query optimization and indexing
   - Database query caching

3. **Caching Layer:**
   - Redis cluster for frequently accessed data
   - Local cache (Caffeine) for hot data
   - Cache warming strategies

4. **Result:**
   - Handled 12M+ conversations/month
   - Reduced infrastructure costs by 40%
   - Achieved 99.9% uptime

---

### Challenge 2: Agent State Consistency

**Problem:**
- Multiple service instances updating agent state
- Race conditions causing incorrect state
- Lost updates
- Inconsistent routing decisions

**Solution:**
1. **Distributed Locks:**
   ```java
   public void updateAgentState(String agentId, AgentStatus newStatus) {
       String lockKey = "lock:agent:" + agentId;
       Boolean lockAcquired = redisTemplate.opsForValue()
           .setIfAbsent(lockKey, "locked", Duration.ofSeconds(10));
       
       if (lockAcquired) {
           try {
               Agent agent = getAgent(agentId);
               agent.setStatus(newStatus);
               saveAgent(agent);
               emitStateChangeEvent(agent);
           } finally {
               redisTemplate.delete(lockKey);
           }
       } else {
           throw new ConcurrentModificationException();
       }
   }
   ```

2. **Optimistic Locking:**
   ```java
   @Entity
   public class Agent {
       @Version
       private Long version; // Optimistic locking
       
       // ...
   }
   ```

3. **Event Sourcing:**
   - All state changes as events
   - Rebuild state from events if needed
   - Event ordering guarantees

**Result:**
- Zero state inconsistencies
- Correct agent routing
- Reliable state management

---

### Challenge 3: NLU Provider Integration Complexity

**Problem:**
- Multiple NLU providers with different APIs
- Provider failures causing service degradation
- High latency from external calls
- Cost optimization needed

**Solution:**
1. **Adapter Pattern:**
   - Unified interface for all providers
   - Provider-specific adapters
   - Common response model

2. **Resilience Patterns:**
   - Circuit breaker (Resilience4j)
   - Retry with exponential backoff
   - Fallback to secondary providers
   - Timeout handling

3. **Caching:**
   - Cache NLU responses
   - Message deduplication
   - Reduced API calls by 30%

4. **Async Processing:**
   - Non-blocking NLU calls
   - Parallel provider calls
   - Improved response time by 50%

**Result:**
- 99.5% NLU service availability
- 50% improvement in response time
- 30% reduction in NLU API costs

---

### Challenge 4: Real-time Message Delivery

**Problem:**
- Messages delayed during peak hours
- WebSocket connection issues
- Message ordering problems
- High latency for real-time chat

**Solution:**
1. **WebSocket Optimization:**
   - Connection pooling
   - Heartbeat mechanism
   - Automatic reconnection
   - Message queuing for offline users

2. **Message Queue:**
   - Kafka for message delivery
   - Partitioning by conversationId
   - Guaranteed message ordering
   - At-least-once delivery

3. **Caching:**
   - Recent messages in Redis
   - Fast message retrieval
   - Reduced database load

**Result:**
- P95 latency reduced by 60%
- 99.9% message delivery success rate
- Real-time message delivery (< 100ms)

---

# Project 2: Prime Broker System - Architecture Deep Dive

## Overview

**Business Context:**
- Prime Broker system for IG Group (financial trading platform)
- Handles trades, instruments, positions, ledger, and settlement
- Event-driven architecture with Kafka
- Scale: 1M+ trades per day with 99.9% accuracy
- Critical financial system requiring high reliability

**Key Requirements:**
- Real-time trade processing
- Accurate position tracking
- Ledger entries for audit
- Settlement processing
- High availability (99.9%+)
- Financial compliance and auditability

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                    Trading Systems                              │
│         (Dealing Platform, Mobile Apps, APIs)                  │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│                    API Gateway                                  │
│              (Rate Limiting, Authentication)                   │
└────────────────────────────┬────────────────────────────────────┘
                             │
                             ↓
┌─────────────────────────────────────────────────────────────────┐
│              Prime Broker Microservices                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │   Trade      │  │  Instrument  │  │  Position    │         │
│  │   Service    │  │   Service   │  │   Service    │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐         │
│  │   Ledger     │  │  Settlement  │  │  Reporting   │         │
│  │   Service    │  │   Service    │  │   Service    │         │
│  └──────────────┘  └──────────────┘  └──────────────┘         │
└────────────────────────────┬────────────────────────────────────┘
                             │
                ┌────────────┼────────────┐
                ↓            ↓            ↓
        ┌─────────────┐ ┌──────────┐ ┌──────────┐
        │   Kafka     │ │ Postgres │ │  Redis   │
        │ Event Bus   │ │   DB     │ │  Cache   │
        └─────────────┘ └──────────┘ └──────────┘
                             │
                             ↓
        ┌─────────────────────────────────────┐
        │     External Systems                │
        │  ┌──────────┐  ┌──────────┐        │
        │  │ Account  │  │ Clearing │        │
        │  │ System   │  │  System  │        │
        │  └──────────┘  └──────────┘        │
        └─────────────────────────────────────┘
```

---

## Core Components Architecture

### 1. Trade Service

**Purpose:**
- Receive and validate trades
- Process trade execution
- Emit trade events
- Maintain trade state

**Architecture:**

```
┌─────────────────────────────────────────────────────────┐
│              Trade Service                               │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  Trade       │  │  Validation  │  │  Event       │  │
│  │  Processor   │  │  Engine      │  │  Publisher   │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  State       │  │  Idempotency │  │  Compensation │  │
│  │  Manager     │  │  Handler     │  │  Handler      │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└────────────────────────────┬─────────────────────────────┘
                             │
                ┌────────────┼────────────┐
                ↓            ↓            ↓
        ┌─────────────┐ ┌──────────┐ ┌──────────┐
        │   Kafka     │ │ Postgres │ │  Redis   │
        │ (trade-     │ │   (Trades)│ │ (Idemp.  │
        │  events)    │ │           │ │  Keys)   │
        └─────────────┘ └──────────┘ └──────────┘
```

**Key Design Decisions:**

1. **Idempotency:**
   ```java
   @Service
   public class TradeService {
       private final RedisTemplate<String, String> redisTemplate;
       private final TradeRepository tradeRepository;
       
       @Transactional
       public Trade processTrade(TradeRequest request) {
           // Check idempotency key
           String idempotencyKey = request.getIdempotencyKey();
           if (idempotencyKey != null) {
               String existingTradeId = redisTemplate.opsForValue()
                   .get("idempotency:" + idempotencyKey);
               if (existingTradeId != null) {
                   return tradeRepository.findById(existingTradeId)
                       .orElseThrow();
               }
           }
           
           // Validate trade
           validateTrade(request);
           
           // Process trade
           Trade trade = createTrade(request);
           tradeRepository.save(trade);
           
           // Store idempotency key
           if (idempotencyKey != null) {
               redisTemplate.opsForValue().set(
                   "idempotency:" + idempotencyKey,
                   trade.getTradeId(),
                   Duration.ofDays(7)
               );
           }
           
           // Emit event
           kafkaTemplate.send("trade-events", trade.getTradeId(), 
               new TradeCreatedEvent(trade));
           
           return trade;
       }
   }
   ```

2. **Event-Driven Processing:**
   ```java
   @EventListener
   public void handleTradeCreated(TradeCreatedEvent event) {
       // Update position
       positionService.updatePosition(event.getTrade());
       
       // Create ledger entry
       ledgerService.createLedgerEntry(event.getTrade());
       
       // Notify settlement service
       settlementService.scheduleSettlement(event.getTrade());
   }
   ```

**Challenges & Solutions:**

**Challenge 1: Trade Processing Order**
- **Problem**: Trades must be processed in order for accurate position calculation
- **Solution**:
  - Kafka partitioning by accountId ensures ordering
  - Single-threaded consumer per partition
  - Sequence numbers for trade ordering

**Challenge 2: Duplicate Trade Prevention**
- **Problem**: Network retries causing duplicate trades
- **Solution**:
  - Idempotency keys in requests
  - Redis-based idempotency check
  - Database unique constraints

---

### 2. Position Service

**Purpose:**
- Track positions for each account/instrument
- Calculate position changes from trades
- Provide real-time position queries
- Maintain position history

**Architecture:**

```
┌─────────────────────────────────────────────────────────┐
│              Position Service                           │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  Position    │  │  Position    │  │  Position    │  │
│  │  Calculator  │  │  Aggregator  │  │  Snapshot    │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└────────────────────────────┬─────────────────────────────┘
                             │
                ┌────────────┼────────────┐
                ↓            ↓            ↓
        ┌─────────────┐ ┌──────────┐ ┌──────────┐
        │   Kafka     │ │ Postgres │ │  Redis   │
        │ (trade-     │ │ (Positions)│ │ (Current │
        │  events)    │ │           │ │ Positions)│
        └─────────────┘ └──────────┘ └──────────┘
```

**Key Design Decisions:**

1. **Event Sourcing for Positions:**
   ```java
   @Service
   public class PositionService {
       public void updatePosition(Trade trade) {
           // Calculate position change
           PositionChange change = calculatePositionChange(trade);
           
           // Update current position (Redis for fast access)
           String key = "position:" + trade.getAccountId() + ":" + trade.getInstrumentId();
           Position currentPosition = getCurrentPosition(trade.getAccountId(), trade.getInstrumentId());
           
           Position newPosition = currentPosition.apply(change);
           redisTemplate.opsForValue().set(key, newPosition);
           
           // Persist position event
           PositionEvent event = new PositionEvent(
               trade.getAccountId(),
               trade.getInstrumentId(),
               change,
               newPosition,
               Instant.now()
           );
           
           kafkaTemplate.send("position-events", event.getAccountId(), event);
           
           // Persist to database (async)
           positionRepository.save(newPosition);
       }
       
       public Position getCurrentPosition(String accountId, String instrumentId) {
           // Try Redis first
           String key = "position:" + accountId + ":" + instrumentId;
           Position position = redisTemplate.opsForValue().get(key);
           
           if (position != null) {
               return position;
           }
           
           // Fallback to database
           position = positionRepository.findByAccountIdAndInstrumentId(accountId, instrumentId)
               .orElse(Position.zero(accountId, instrumentId));
           
           // Cache in Redis
           redisTemplate.opsForValue().set(key, position);
           
           return position;
       }
   }
   ```

2. **Position Snapshot:**
   ```java
   @Scheduled(cron = "0 0 * * * *") // Every hour
   public void createPositionSnapshot() {
       // Create snapshot of all positions
       List<Position> positions = getAllCurrentPositions();
       
       PositionSnapshot snapshot = new PositionSnapshot(
           Instant.now(),
           positions
       );
       
       snapshotRepository.save(snapshot);
   }
   ```

**Challenges & Solutions:**

**Challenge 1: Position Calculation Accuracy**
- **Problem**: Position calculations must be 100% accurate for financial compliance
- **Solution**:
  - Event sourcing for audit trail
  - Double-entry bookkeeping
  - Reconciliation jobs
  - Position snapshots for verification

**Challenge 2: High Query Volume**
- **Problem**: Real-time position queries causing database load
- **Solution**:
  - Redis cache for current positions
  - Read replicas for queries
  - Position snapshots for historical queries
  - Query optimization and indexing

---

### 3. Ledger Service

**Purpose:**
- Create ledger entries for all financial transactions
- Maintain audit trail
- Support double-entry bookkeeping
- Enable financial reporting

**Architecture:**

```
┌─────────────────────────────────────────────────────────┐
│              Ledger Service                             │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  Ledger      │  │  Double-Entry│  │  Reconciliation│ │
│  │  Entry       │  │  Validator   │  │  Engine       │  │
│  │  Creator     │  │              │  │               │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└────────────────────────────┬─────────────────────────────┘
                             │
                ┌────────────┼────────────┐
                ↓            ↓            ↓
        ┌─────────────┐ ┌──────────┐ ┌──────────┐
        │   Kafka     │ │ Postgres │ │  Redis   │
        │ (ledger-    │ │ (Ledger  │ │ (Recent  │
        │  events)    │ │ Entries) │ │ Entries) │
        └─────────────┘ └──────────┘ └──────────┘
```

**Key Design Decisions:**

1. **Double-Entry Bookkeeping:**
   ```java
   @Service
   public class LedgerService {
       @Transactional
       public void createLedgerEntry(Trade trade) {
           // Debit entry
           LedgerEntry debitEntry = LedgerEntry.builder()
               .accountId(trade.getAccountId())
               .instrumentId(trade.getInstrumentId())
               .entryType(EntryType.DEBIT)
               .amount(trade.getQuantity())
               .currency(trade.getCurrency())
               .tradeId(trade.getTradeId())
               .timestamp(Instant.now())
               .build();
           
           // Credit entry
           LedgerEntry creditEntry = LedgerEntry.builder()
               .accountId(trade.getCounterpartyAccountId())
               .instrumentId(trade.getInstrumentId())
               .entryType(EntryType.CREDIT)
               .amount(trade.getQuantity())
               .currency(trade.getCurrency())
               .tradeId(trade.getTradeId())
               .timestamp(Instant.now())
               .build();
           
           // Validate double-entry
           validateDoubleEntry(debitEntry, creditEntry);
           
           // Save both entries atomically
           ledgerRepository.saveAll(List.of(debitEntry, creditEntry));
           
           // Emit event
           kafkaTemplate.send("ledger-events", 
               new LedgerEntryCreatedEvent(debitEntry, creditEntry));
       }
       
       private void validateDoubleEntry(LedgerEntry debit, LedgerEntry credit) {
           if (!debit.getAmount().equals(credit.getAmount())) {
               throw new InvalidLedgerEntryException("Debit and credit amounts must match");
           }
           if (debit.getCurrency() != credit.getCurrency()) {
               throw new InvalidLedgerEntryException("Debit and credit currencies must match");
           }
       }
   }
   ```

2. **Ledger Reconciliation:**
   ```java
   @Scheduled(cron = "0 0 2 * * *") // Daily at 2 AM
   public void reconcileLedger() {
       // Get all ledger entries for yesterday
       LocalDate yesterday = LocalDate.now().minusDays(1);
       List<LedgerEntry> entries = ledgerRepository.findByDate(yesterday);
       
       // Group by account and instrument
       Map<String, BigDecimal> balances = entries.stream()
           .collect(Collectors.groupingBy(
               entry -> entry.getAccountId() + ":" + entry.getInstrumentId(),
               Collectors.reducing(
                   BigDecimal.ZERO,
                   entry -> entry.getEntryType() == EntryType.DEBIT 
                       ? entry.getAmount() 
                       : entry.getAmount().negate(),
                   BigDecimal::add
               )
           ));
       
       // Compare with position balances
       for (Map.Entry<String, BigDecimal> balance : balances.entrySet()) {
           String[] parts = balance.getKey().split(":");
           String accountId = parts[0];
           String instrumentId = parts[1];
           
           Position position = positionService.getCurrentPosition(accountId, instrumentId);
           
           if (!position.getBalance().equals(balance.getValue())) {
               // Reconciliation failure - alert
               alertService.sendReconciliationAlert(accountId, instrumentId, 
                   position.getBalance(), balance.getValue());
           }
       }
   }
   ```

**Challenges & Solutions:**

**Challenge 1: High Volume of Ledger Entries**
- **Problem**: 400K+ ledger entries per day causing database load
- **Solution**:
  - Batch inserts
  - Database partitioning by date
  - Async processing
  - Archival strategy for old entries

**Challenge 2: Ledger Integrity**
- **Problem**: Must ensure all entries are balanced (double-entry)
- **Solution**:
  - Database constraints
  - Validation before save
  - Reconciliation jobs
  - Audit logging

---

### 4. Settlement Service

**Purpose:**
- Process trade settlements
- Coordinate with clearing systems
- Handle settlement failures
- Maintain settlement status

**Architecture:**

```
┌─────────────────────────────────────────────────────────┐
│              Settlement Service                          │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  Settlement  │  │  Clearing    │  │  Failure     │  │
│  │  Processor   │  │  Adapter     │  │  Handler     │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │  Retry       │  │  Notification│  │  Compensation│  │
│  │  Manager     │  │  Service     │  │  Handler     │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└────────────────────────────┬─────────────────────────────┘
                             │
                ┌────────────┼────────────┐
                ↓            ↓            ↓
        ┌─────────────┐ ┌──────────┐ ┌──────────┐
        │   Kafka     │ │ Postgres │ │ External │
        │ (settlement-│ │ (Settlement│ │ Clearing │
        │  events)    │ │  Status)  │ │  System  │
        └─────────────┘ └──────────┘ └──────────┘
```

**Key Design Decisions:**

1. **Saga Pattern for Settlement:**
   ```java
   @Service
   public class SettlementService {
       public void processSettlement(Trade trade) {
           SettlementSaga saga = new SettlementSaga(trade);
           
           try {
               // Step 1: Validate settlement requirements
               saga.validateSettlement();
               
               // Step 2: Call clearing system
               ClearingResponse response = clearingAdapter.settle(trade);
               saga.setClearingResponse(response);
               
               // Step 3: Update settlement status
               saga.markAsSettled();
               
               // Step 4: Notify relevant systems
               notificationService.notifySettlementComplete(trade);
               
           } catch (Exception e) {
               // Compensate
               saga.compensate();
               throw new SettlementException("Settlement failed", e);
           }
       }
   }
   ```

2. **Retry Logic:**
   ```java
   @Retryable(value = {ClearingSystemException.class}, maxAttempts = 3)
   public ClearingResponse settleTrade(Trade trade) {
       return clearingAdapter.settle(trade);
   }
   ```

**Challenges & Solutions:**

**Challenge 1: Settlement Failures**
- **Problem**: External clearing system failures causing settlement issues
- **Solution**:
  - Retry with exponential backoff
  - Circuit breaker pattern
  - Manual intervention workflow
  - Compensation transactions

**Challenge 2: Settlement Timing**
- **Problem**: Trades must settle at specific times (T+0, T+1, T+2)
- **Solution**:
  - Scheduled jobs for settlement windows
  - Time-based event triggers
  - Settlement calendar management

---

## Event-Driven Architecture

### Kafka Event Bus

**Event Topics:**
- `trade-events` - Trade created, updated, cancelled
- `position-events` - Position changes
- `ledger-events` - Ledger entries created
- `settlement-events` - Settlement status changes
- `instrument-events` - Instrument updates

**Event Schema:**
```java
public class TradeCreatedEvent {
    private String tradeId;
    private String accountId;
    private String instrumentId;
    private BigDecimal quantity;
    private BigDecimal price;
    private String currency;
    private TradeType type;
    private Instant timestamp;
    private String idempotencyKey;
}
```

**Event Processing:**
```java
@KafkaListener(topics = "trade-events", groupId = "position-service")
public void handleTradeEvent(TradeCreatedEvent event) {
    // Update position
    positionService.updatePosition(event);
}

@KafkaListener(topics = "trade-events", groupId = "ledger-service")
public void handleTradeEvent(TradeCreatedEvent event) {
    // Create ledger entry
    ledgerService.createLedgerEntry(event);
}
```

---

## Challenges Faced & Solutions

### Challenge 1: Handling 1M+ Trades Per Day

**Problem:**
- System couldn't handle high trade volume
- Database bottlenecks
- Slow position calculations
- Event processing delays

**Solution:**
1. **Horizontal Scaling:**
   - Multiple service instances
   - Kafka consumer groups for parallel processing
   - Database read replicas

2. **Optimization:**
   - Batch processing for ledger entries
   - Redis caching for positions
   - Database indexing
   - Connection pooling

3. **Async Processing:**
   - Async event handlers
   - Non-blocking I/O
   - Background jobs for heavy operations

**Result:**
- Handled 1M+ trades/day
- 99.9% accuracy
- Sub-second position updates

---

### Challenge 2: Financial Accuracy & Compliance

**Problem:**
- Position calculations must be 100% accurate
- Regulatory compliance requirements
- Audit trail needed
- Reconciliation challenges

**Solution:**
1. **Event Sourcing:**
   - All changes as events
   - Rebuild state from events
   - Complete audit trail

2. **Double-Entry Bookkeeping:**
   - Every transaction has debit and credit
   - Validation before save
   - Automatic reconciliation

3. **Reconciliation Jobs:**
   - Daily reconciliation
   - Position vs ledger balance
   - Alert on discrepancies

**Result:**
- 100% accuracy
- Complete audit trail
- Regulatory compliance

---

### Challenge 3: Event Ordering & Consistency

**Problem:**
- Events processed out of order
- Position calculations incorrect
- Ledger entries unbalanced
- Data inconsistency

**Solution:**
1. **Kafka Partitioning:**
   - Partition by accountId
   - Ensures ordering per account
   - Single consumer per partition

2. **Sequence Numbers:**
   - Sequence number in events
   - Detect out-of-order events
   - Reorder if needed

3. **Idempotency:**
   - Idempotent event handlers
   - Deduplication
   - Exactly-once processing

**Result:**
- Correct event ordering
- Consistent state
- Accurate calculations

---

### Challenge 4: High Availability

**Problem:**
- System downtime unacceptable
- Single points of failure
- Database failures
- Service crashes

**Solution:**
1. **Redundancy:**
   - Multiple service instances
   - Database replication
   - Kafka cluster
   - Redis cluster

2. **Health Checks:**
   - Liveness probes
   - Readiness probes
   - Automatic restart
   - Circuit breakers

3. **Disaster Recovery:**
   - Database backups
   - Event replay capability
   - Multi-region deployment (future)

**Result:**
- 99.9%+ uptime
- Automatic failover
- Zero data loss

---

## Performance Metrics

### Before Optimization:
- Trade processing: 500ms average
- Position query: 200ms average
- Ledger entry creation: 100ms average
- System capacity: 500K trades/day

### After Optimization:
- Trade processing: 50ms average (10x improvement)
- Position query: 10ms average (20x improvement)
- Ledger entry creation: 20ms average (5x improvement)
- System capacity: 1M+ trades/day (2x improvement)

---

## Key Learnings

1. **Event-Driven Architecture**: Essential for financial systems requiring audit trails
2. **Idempotency**: Critical for handling retries and failures
3. **Caching Strategy**: Multi-level caching significantly improves performance
4. **Event Ordering**: Partitioning strategy crucial for consistency
5. **Financial Accuracy**: Double-entry bookkeeping and reconciliation are mandatory
6. **Scalability**: Horizontal scaling with stateless services enables growth
7. **Monitoring**: Comprehensive observability essential for financial systems

---

**Both projects demonstrate expertise in:**
- Microservices architecture
- Event-driven systems
- High-scale distributed systems
- Financial system design
- Performance optimization
- System reliability

