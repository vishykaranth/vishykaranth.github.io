# EP03: Load Balancer Basics, Usage and Configuration - Complete Guide

## Table of Contents
1. [Load Balancer Basics](#load-balancer-basics)
2. [Why Use Load Balancers?](#why-use-load-balancers)
3. [Types of Load Balancers](#types-of-load-balancers)
4. [Load Balancing Algorithms](#load-balancing-algorithms)
5. [Load Balancer Architecture](#load-balancer-architecture)
6. [Configuration Examples](#configuration-examples)
7. [High Availability Setup](#high-availability-setup)
8. [Health Checks](#health-checks)
9. [SSL/TLS Termination](#ssltls-termination)
10. [Session Persistence](#session-persistence)
11. [Common Use Cases](#common-use-cases)
12. [Best Practices](#best-practices)
13. [Troubleshooting](#troubleshooting)
14. [Interview Questions & Answers](#interview-questions--answers)

---

## Load Balancer Basics

### What is a Load Balancer?

**Load Balancer** is:
- **Traffic Distributor**: Distributes incoming traffic across multiple servers
- **High Availability**: Ensures service availability if servers fail
- **Performance Optimizer**: Improves performance by distributing load
- **Scalability Enabler**: Enables horizontal scaling
- **Single Entry Point**: Provides single entry point for clients

### Key Concepts

**1. Load Distribution**
```
Client Request → Load Balancer → Server 1
                              → Server 2
                              → Server 3
```

**2. Health Monitoring**
```
Load Balancer → Health Check → Server 1 (Healthy)
                            → Server 2 (Unhealthy) → Remove from pool
                            → Server 3 (Healthy)
```

**3. Session Persistence**
```
Client → Load Balancer → Server 1 (Sticky Session)
      → Same Client → Load Balancer → Server 1 (Same server)
```

---

## Why Use Load Balancers?

### 1. High Availability

**Problem Without Load Balancer:**
```
Single Server → If fails → Service Down
```

**Solution With Load Balancer:**
```
Load Balancer → Server 1 (If fails)
             → Server 2 (Takes over)
             → Server 3 (Backup)
```

### 2. Scalability

**Horizontal Scaling:**
```
1 Server → 2 Servers → 3 Servers → N Servers
Load Balancer distributes traffic across all
```

### 3. Performance

**Load Distribution:**
```
1000 requests → 1 Server (Overloaded)
1000 requests → 10 Servers (100 requests each) → Better performance
```

### 4. Flexibility

**Server Management:**
```
Add servers → Load Balancer automatically includes
Remove servers → Load Balancer automatically excludes
Maintenance → Remove server from pool, perform maintenance, add back
```

### 5. Security

**Benefits:**
- **DDoS Protection**: Load balancer can filter malicious traffic
- **SSL Termination**: Handle SSL/TLS at load balancer
- **IP Whitelisting**: Control access at load balancer
- **Rate Limiting**: Implement rate limiting at load balancer

---

## Types of Load Balancers

### 1. Layer 4 (Transport Layer) Load Balancer

**Characteristics:**
- **OSI Layer**: Layer 4 (Transport)
- **Protocols**: TCP, UDP
- **Decision Based On**: IP address and port
- **Performance**: Fast, low overhead
- **Use Cases**: Simple load balancing, high throughput

**How It Works:**
```
Client → Load Balancer (IP:Port) → Server 1 (IP:Port)
                                → Server 2 (IP:Port)
```

**Example:**
```
Client connects to: 10.0.0.1:80
Load Balancer forwards to:
  - Server 1: 192.168.1.10:8080
  - Server 2: 192.168.1.11:8080
  - Server 3: 192.168.1.12:8080
```

**Pros:**
- **Fast**: Low latency, high throughput
- **Simple**: Simple configuration
- **Efficient**: Low CPU/memory usage

**Cons:**
- **Limited**: Can't route based on content
- **No SSL**: Can't inspect SSL content
- **Basic**: Limited features

### 2. Layer 7 (Application Layer) Load Balancer

**Characteristics:**
- **OSI Layer**: Layer 7 (Application)
- **Protocols**: HTTP, HTTPS
- **Decision Based On**: URL, headers, cookies, content
- **Performance**: Slower than L4, more features
- **Use Cases**: Content-based routing, SSL termination

**How It Works:**
```
Client → Load Balancer (Inspects HTTP) → Server 1 (/api/users)
                                      → Server 2 (/api/orders)
                                      → Server 3 (/static)
```

**Example:**
```
Request: GET /api/users HTTP/1.1
Load Balancer routes to: User Service

Request: GET /api/orders HTTP/1.1
Load Balancer routes to: Order Service

Request: GET /static/logo.png HTTP/1.1
Load Balancer routes to: Static File Server
```

**Pros:**
- **Flexible**: Content-based routing
- **SSL Termination**: Handle SSL at load balancer
- **Advanced Features**: URL rewriting, header manipulation
- **Intelligent Routing**: Route based on content

**Cons:**
- **Slower**: Higher latency than L4
- **More CPU**: More CPU intensive
- **Complex**: More complex configuration

### 3. Hardware Load Balancer

**Characteristics:**
- **Type**: Physical appliance
- **Performance**: Very high performance
- **Cost**: Expensive
- **Maintenance**: Requires physical maintenance
- **Examples**: F5 BIG-IP, Citrix NetScaler

**Use Cases:**
- **Enterprise**: Large enterprises
- **High Performance**: Very high throughput requirements
- **Dedicated**: Dedicated infrastructure

### 4. Software Load Balancer

**Characteristics:**
- **Type**: Software application
- **Performance**: Good performance
- **Cost**: Lower cost
- **Flexibility**: Easy to deploy and scale
- **Examples**: NGINX, HAProxy, AWS ELB

**Use Cases:**
- **Cloud**: Cloud deployments
- **Flexibility**: Need flexibility
- **Cost-Effective**: Cost-effective solution

### 5. Cloud Load Balancer

**Characteristics:**
- **Type**: Managed service
- **Provider**: Cloud provider (AWS, GCP, Azure)
- **Management**: Fully managed
- **Scaling**: Auto-scaling
- **Examples**: AWS ELB, GCP Load Balancer, Azure Load Balancer

**Use Cases:**
- **Cloud Native**: Cloud-native applications
- **Managed Service**: Want managed service
- **Auto-Scaling**: Need auto-scaling

---

## Load Balancing Algorithms

### 1. Round Robin

**Algorithm:**
```
Request 1 → Server 1
Request 2 → Server 2
Request 3 → Server 3
Request 4 → Server 1 (cycle)
```

**Configuration (NGINX):**
```nginx
upstream backend {
    server server1.example.com;
    server server2.example.com;
    server server3.example.com;
    # Round robin is default
}
```

**Pros:**
- **Simple**: Simple to implement
- **Fair**: Distributes evenly
- **Predictable**: Predictable behavior

**Cons:**
- **No Consideration**: Doesn't consider server load
- **Equal Distribution**: Assumes equal server capacity

### 2. Weighted Round Robin

**Algorithm:**
```
Server 1 (weight: 3) → 3 requests
Server 2 (weight: 1) → 1 request
Server 3 (weight: 2) → 2 requests
```

**Configuration (NGINX):**
```nginx
upstream backend {
    server server1.example.com weight=3;
    server server2.example.com weight=1;
    server server3.example.com weight=2;
}
```

**Use Cases:**
- **Different Capacity**: Servers with different capacities
- **Gradual Migration**: Gradually migrate traffic
- **Testing**: Test new servers with less traffic

### 3. Least Connections

**Algorithm:**
```
Request → Server with fewest active connections
```

**Configuration (NGINX):**
```nginx
upstream backend {
    least_conn;
    server server1.example.com;
    server server2.example.com;
    server server3.example.com;
}
```

**Use Cases:**
- **Long Connections**: Long-lived connections
- **Variable Load**: Variable request processing time
- **Fair Distribution**: Fair distribution based on load

### 4. IP Hash

**Algorithm:**
```
Hash(client IP) → Consistent server assignment
```

**Configuration (NGINX):**
```nginx
upstream backend {
    ip_hash;
    server server1.example.com;
    server server2.example.com;
    server server3.example.com;
}
```

**Use Cases:**
- **Session Persistence**: Need session persistence
- **Sticky Sessions**: Sticky sessions without cookies
- **Cache Affinity**: Cache affinity

### 5. Least Response Time

**Algorithm:**
```
Request → Server with lowest response time
```

**Configuration (NGINX Plus):**
```nginx
upstream backend {
    least_time header;
    server server1.example.com;
    server server2.example.com;
    server server3.example.com;
}
```

**Use Cases:**
- **Performance**: Optimize for performance
- **Response Time**: Minimize response time
- **Dynamic Load**: Dynamic server load

### 6. Random

**Algorithm:**
```
Request → Random server
```

**Use Cases:**
- **Testing**: Testing scenarios
- **Simple**: Simple load distribution
- **No Preference**: No preference for servers

---

## Load Balancer Architecture

### 1. Basic Architecture

```
Internet
   ↓
Load Balancer
   ↓
┌──────────┬──────────┬──────────┐
│ Server 1 │ Server 2 │ Server 3 │
└──────────┴──────────┴──────────┘
```

### 2. Multi-Tier Architecture

```
Internet
   ↓
Load Balancer (L7)
   ↓
┌──────────┬──────────┬──────────┐
│ App 1    │ App 2    │ App 3    │
└──────────┴──────────┴──────────┘
   ↓
Load Balancer (L4)
   ↓
┌──────────┬──────────┬──────────┐
│ DB 1     │ DB 2     │ DB 3     │
└──────────┴──────────┴──────────┘
```

### 3. Active-Passive Architecture

```
Internet
   ↓
┌──────────────┬──────────────┐
│ Load Balancer│ Load Balancer│
│   (Active)   │   (Passive)  │
└──────────────┴──────────────┘
   ↓
┌──────────┬──────────┬──────────┐
│ Server 1 │ Server 2 │ Server 3 │
└──────────┴──────────┴──────────┘
```

### 4. Active-Active Architecture

```
Internet
   ↓
┌──────────────┬──────────────┐
│ Load Balancer│ Load Balancer│
│   (Active)   │   (Active)   │
└──────────────┴──────────────┘
   ↓              ↓
┌──────────┬──────────┬──────────┐
│ Server 1 │ Server 2 │ Server 3 │
└──────────┴──────────┴──────────┘
```

---

## Configuration Examples

### 1. NGINX Configuration

**Basic Configuration:**
```nginx
http {
    upstream backend {
        server 192.168.1.10:8080;
        server 192.168.1.11:8080;
        server 192.168.1.12:8080;
    }

    server {
        listen 80;
        server_name example.com;

        location / {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
```

**Advanced Configuration:**
```nginx
http {
    upstream backend {
        least_conn;
        server 192.168.1.10:8080 weight=3 max_fails=3 fail_timeout=30s;
        server 192.168.1.11:8080 weight=2 max_fails=3 fail_timeout=30s;
        server 192.168.1.12:8080 weight=1 max_fails=3 fail_timeout=30s backup;
        
        keepalive 32;
    }

    server {
        listen 80;
        server_name example.com;

        # Health check endpoint
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }

        location / {
            proxy_pass http://backend;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # Buffering
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }
    }
}
```

### 2. HAProxy Configuration

**Basic Configuration:**
```haproxy
global
    log /dev/log local0
    maxconn 4096
    daemon

defaults
    log global
    mode http
    option httplog
    option dontlognull
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

frontend http-in
    bind *:80
    default_backend servers

backend servers
    balance roundrobin
    server server1 192.168.1.10:8080 check
    server server2 192.168.1.11:8080 check
    server server3 192.168.1.12:8080 check
```

**Advanced Configuration:**
```haproxy
global
    log /dev/log local0
    maxconn 4096
    daemon

defaults
    log global
    mode http
    option httplog
    option dontlognull
    option forwardfor
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

frontend http-in
    bind *:80
    bind *:443 ssl crt /etc/ssl/certs/example.com.pem
    
    # Redirect HTTP to HTTPS
    redirect scheme https code 301 if !{ ssl_fc }
    
    # ACLs for routing
    acl is_api path_beg /api
    acl is_static path_beg /static
    
    use_backend api_servers if is_api
    use_backend static_servers if is_static
    default_backend web_servers

backend web_servers
    balance leastconn
    option httpchk GET /health
    http-check expect status 200
    cookie SERVERID insert indirect nocache
    server server1 192.168.1.10:8080 check cookie s1
    server server2 192.168.1.11:8080 check cookie s2
    server server3 192.168.1.12:8080 check cookie s3 backup

backend api_servers
    balance roundrobin
    option httpchk GET /api/health
    http-check expect status 200
    server api1 192.168.1.20:8080 check
    server api2 192.168.1.21:8080 check

backend static_servers
    balance roundrobin
    server static1 192.168.1.30:8080 check
    server static2 192.168.1.31:8080 check
```

### 3. AWS Application Load Balancer (ALB)

**Terraform Configuration:**
```hcl
resource "aws_lb" "main" {
  name               = "main-lb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.lb.id]
  subnets            = [aws_subnet.public.*.id]

  enable_deletion_protection = false
  enable_http2                = true
  enable_cross_zone_load_balancing = true

  tags = {
    Environment = "production"
  }
}

resource "aws_lb_target_group" "app" {
  name     = "app-targets"
  port     = 8080
  protocol = "HTTP"
  vpc_id   = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    unhealthy_threshold = 2
    timeout             = 5
    interval            = 30
    path                = "/health"
    protocol            = "HTTP"
    matcher             = "200"
  }

  stickiness {
    enabled = true
    type    = "lb_cookie"
    cookie_duration = 86400
  }
}

resource "aws_lb_listener" "app" {
  load_balancer_arn = aws_lb.main.arn
  port              = "80"
  protocol          = "HTTP"

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.app.arn
  }
}

resource "aws_lb_listener_rule" "api" {
  listener_arn = aws_lb_listener.app.arn
  priority     = 100

  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.api.arn
  }

  condition {
    path_pattern {
      values = ["/api/*"]
    }
  }
}
```

**CloudFormation Configuration:**
```yaml
Resources:
  ApplicationLoadBalancer:
    Type: AWS::ElasticLoadBalancingV2::LoadBalancer
    Properties:
      Name: main-lb
      Type: application
      Scheme: internet-facing
      Subnets:
        - !Ref PublicSubnet1
        - !Ref PublicSubnet2
      SecurityGroups:
        - !Ref LoadBalancerSecurityGroup
      LoadBalancerAttributes:
        - Key: idle_timeout.timeout_seconds
          Value: 60
        - Key: enable_http2
          Value: true

  TargetGroup:
    Type: AWS::ElasticLoadBalancingV2::TargetGroup
    Properties:
      Name: app-targets
      Port: 8080
      Protocol: HTTP
      VpcId: !Ref VPC
      HealthCheckPath: /health
      HealthCheckIntervalSeconds: 30
      HealthCheckTimeoutSeconds: 5
      HealthyThresholdCount: 2
      UnhealthyThresholdCount: 2
      TargetGroupAttributes:
        - Key: stickiness.enabled
          Value: true
        - Key: stickiness.type
          Value: lb_cookie
        - Key: stickiness.lb_cookie.duration_seconds
          Value: 86400

  Listener:
    Type: AWS::ElasticLoadBalancingV2::Listener
    Properties:
      LoadBalancerArn: !Ref ApplicationLoadBalancer
      Port: 80
      Protocol: HTTP
      DefaultActions:
        - Type: forward
          TargetGroupArn: !Ref TargetGroup
```

### 4. Kubernetes Ingress

**NGINX Ingress Configuration:**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
    nginx.ingress.kubernetes.io/load-balance: "least_conn"
spec:
  tls:
    - hosts:
        - example.com
      secretName: tls-secret
  rules:
    - host: example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: app-service
                port:
                  number: 80
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: api-service
                port:
                  number: 80
```

---

## High Availability Setup

### 1. Active-Passive Setup

**Architecture:**
```
Internet
   ↓
┌──────────────┬──────────────┐
│ Load Balancer│ Load Balancer│
│   (Active)   │   (Passive)  │
│   VIP: 10.0.0.1              │
└──────────────┴──────────────┘
   ↓
┌──────────┬──────────┬──────────┐
│ Server 1 │ Server 2 │ Server 3 │
└──────────┴──────────┴──────────┘
```

**Keepalived Configuration:**
```conf
# /etc/keepalived/keepalived.conf

vrrp_script chk_nginx {
    script "/usr/bin/curl -f http://localhost/health || exit 1"
    interval 2
    weight -2
    fall 3
    rise 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 101
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass password123
    }
    virtual_ipaddress {
        10.0.0.1/24
    }
    track_script {
        chk_nginx
    }
}
```

### 2. Active-Active Setup

**Architecture:**
```
Internet
   ↓
DNS (Round Robin)
   ↓
┌──────────────┬──────────────┐
│ Load Balancer│ Load Balancer│
│   (Active)   │   (Active)   │
│   10.0.0.1   │   10.0.0.2   │
└──────────────┴──────────────┘
   ↓              ↓
┌──────────┬──────────┬──────────┐
│ Server 1 │ Server 2 │ Server 3 │
└──────────┴──────────┴──────────┘
```

---

## Health Checks

### 1. HTTP Health Check

**Configuration (NGINX):**
```nginx
upstream backend {
    server 192.168.1.10:8080;
    server 192.168.1.11:8080;
}

server {
    location /health {
        proxy_pass http://backend;
        proxy_set_header Host $host;
    }
}
```

**Health Check Endpoint:**
```java
@RestController
public class HealthController {
    
    @GetMapping("/health")
    public ResponseEntity<Map<String, String>> health() {
        Map<String, String> status = new HashMap<>();
        status.put("status", "UP");
        status.put("timestamp", Instant.now().toString());
        return ResponseEntity.ok(status);
    }
    
    @GetMapping("/health/readiness")
    public ResponseEntity<Map<String, String>> readiness() {
        // Check database connection
        boolean dbHealthy = checkDatabase();
        
        if (dbHealthy) {
            return ResponseEntity.ok(Map.of("status", "READY"));
        } else {
            return ResponseEntity.status(503)
                    .body(Map.of("status", "NOT_READY"));
        }
    }
    
    @GetMapping("/health/liveness")
    public ResponseEntity<Map<String, String>> liveness() {
        return ResponseEntity.ok(Map.of("status", "ALIVE"));
    }
}
```

### 2. TCP Health Check

**Configuration (HAProxy):**
```haproxy
backend servers
    option tcp-check
    tcp-check connect
    tcp-check send "PING\r\n"
    tcp-check expect string "PONG"
    server server1 192.168.1.10:8080 check
    server server2 192.168.1.11:8080 check
```

### 3. Custom Health Check

**Configuration:**
```nginx
upstream backend {
    server 192.168.1.10:8080;
    server 192.168.1.11:8080;
}

server {
    location /health {
        access_log off;
        proxy_pass http://backend/health;
        proxy_set_header Host $host;
        
        # Custom health check logic
        if ($upstream_response_time > 1) {
            return 503;
        }
    }
}
```

---

## SSL/TLS Termination

### 1. SSL Termination at Load Balancer

**Architecture:**
```
Client (HTTPS) → Load Balancer (SSL Termination) → Backend (HTTP)
```

**NGINX Configuration:**
```nginx
server {
    listen 443 ssl http2;
    server_name example.com;
    
    ssl_certificate /etc/ssl/certs/example.com.crt;
    ssl_certificate_key /etc/ssl/private/example.com.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    
    location / {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Real-IP $remote_addr;
    }
}

# Redirect HTTP to HTTPS
server {
    listen 80;
    server_name example.com;
    return 301 https://$server_name$request_uri;
}
```

### 2. End-to-End SSL

**Architecture:**
```
Client (HTTPS) → Load Balancer (HTTPS) → Backend (HTTPS)
```

**Configuration:**
```nginx
server {
    listen 443 ssl;
    server_name example.com;
    
    ssl_certificate /etc/ssl/certs/example.com.crt;
    ssl_certificate_key /etc/ssl/private/example.com.key;
    
    location / {
        proxy_pass https://backend;
        proxy_ssl_verify off;
        proxy_set_header Host $host;
    }
}
```

---

## Session Persistence

### 1. Cookie-Based Sticky Sessions

**NGINX Configuration:**
```nginx
upstream backend {
    ip_hash;  # Or use sticky cookie
    server 192.168.1.10:8080;
    server 192.168.1.11:8080;
    server 192.168.1.12:8080;
}
```

**HAProxy Configuration:**
```haproxy
backend servers
    balance roundrobin
    cookie SERVERID insert indirect nocache
    server server1 192.168.1.10:8080 check cookie s1
    server server2 192.168.1.11:8080 check cookie s2
    server server3 192.168.1.12:8080 check cookie s3
```

### 2. IP Hash Sticky Sessions

**NGINX Configuration:**
```nginx
upstream backend {
    ip_hash;
    server 192.168.1.10:8080;
    server 192.168.1.11:8080;
    server 192.168.1.12:8080;
}
```

**Limitations:**
- **NAT**: Multiple clients behind NAT get same server
- **IP Changes**: Client IP changes break stickiness
- **Load Imbalance**: May cause load imbalance

### 3. Application-Level Session Management

**Shared Session Store:**
```java
// Use Redis for shared sessions
@Configuration
public class SessionConfig {
    
    @Bean
    public RedisConnectionFactory connectionFactory() {
        return new JedisConnectionFactory();
    }
    
    @Bean
    public RedisTemplate<String, Object> redisTemplate() {
        RedisTemplate<String, Object> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory());
        return template;
    }
}
```

---

## Common Use Cases

### 1. Web Application Load Balancing

**Architecture:**
```
Internet → Load Balancer → Web Servers (Nginx/Apache)
                        → App Servers (Tomcat/Node.js)
                        → Database (MySQL/PostgreSQL)
```

### 2. API Load Balancing

**Architecture:**
```
API Clients → API Gateway → Load Balancer → API Servers
```

### 3. Database Load Balancing

**Architecture:**
```
Application → Load Balancer → Master DB (Write)
                          → Replica DBs (Read)
```

### 4. Microservices Load Balancing

**Architecture:**
```
Client → API Gateway → Service Mesh → Microservices
```

---

## Best Practices

### 1. Health Checks

**Best Practices:**
- **Regular Checks**: Check health regularly
- **Fast Failures**: Fail fast on unhealthy servers
- **Graceful Recovery**: Allow time for recovery
- **Separate Endpoints**: Use separate endpoints for liveness/readiness

### 2. Timeouts

**Configuration:**
```nginx
proxy_connect_timeout 60s;
proxy_send_timeout 60s;
proxy_read_timeout 60s;
```

### 3. Connection Pooling

**Configuration:**
```nginx
upstream backend {
    keepalive 32;
    server 192.168.1.10:8080;
}
```

### 4. Monitoring

**Metrics to Monitor:**
- **Request Rate**: Requests per second
- **Response Time**: Average response time
- **Error Rate**: Error percentage
- **Server Health**: Healthy/unhealthy servers
- **Connection Count**: Active connections

### 5. Security

**Security Best Practices:**
- **SSL/TLS**: Use HTTPS
- **Rate Limiting**: Implement rate limiting
- **IP Whitelisting**: Whitelist allowed IPs
- **DDoS Protection**: Protect against DDoS
- **WAF**: Web Application Firewall

---

## Troubleshooting

### 1. Server Not Receiving Traffic

**Check:**
- Health check status
- Server is in pool
- Load balancer configuration
- Network connectivity

### 2. High Latency

**Check:**
- Server performance
- Network latency
- Load balancer algorithm
- Connection pooling

### 3. Session Issues

**Check:**
- Session persistence configuration
- Cookie settings
- Shared session store
- Sticky session configuration

### 4. SSL/TLS Issues

**Check:**
- Certificate validity
- SSL/TLS configuration
- Cipher suites
- Certificate chain

---

## Interview Questions & Answers

### Q1: What is a load balancer?

**Answer:**
- **Traffic Distributor**: Distributes incoming traffic across multiple servers
- **High Availability**: Ensures service availability
- **Performance**: Improves performance by distributing load
- **Scalability**: Enables horizontal scaling
- **Single Entry Point**: Provides single entry point

### Q2: What is the difference between Layer 4 and Layer 7 load balancers?

**Answer:**
- **Layer 4**: TCP/UDP, IP and port based, faster, simpler
- **Layer 7**: HTTP/HTTPS, content based, slower, more features
- **Layer 4**: Can't inspect content
- **Layer 7**: Can route based on URL, headers, content

### Q3: What are common load balancing algorithms?

**Answer:**
1. **Round Robin**: Distribute requests evenly
2. **Weighted Round Robin**: Distribute based on weight
3. **Least Connections**: Route to server with fewest connections
4. **IP Hash**: Route based on client IP hash
5. **Least Response Time**: Route to server with lowest response time

### Q4: How do you ensure high availability for load balancers?

**Answer:**
- **Active-Passive**: Primary and standby load balancers
- **Active-Active**: Multiple active load balancers
- **Health Checks**: Monitor load balancer health
- **Failover**: Automatic failover on failure
- **DNS**: Use DNS for load balancer failover

### Q5: What are health checks and why are they important?

**Answer:**
- **Health Checks**: Monitor server health
- **Remove Unhealthy**: Remove unhealthy servers from pool
- **Add Healthy**: Add recovered servers back
- **Prevent Failures**: Prevent routing to failed servers
- **Types**: HTTP, TCP, custom health checks

### Q6: What is SSL termination?

**Answer:**
- **SSL Termination**: Decrypt SSL at load balancer
- **Backend HTTP**: Backend servers use HTTP
- **Performance**: Reduces backend server load
- **Centralized**: Centralized SSL management
- **Security**: SSL handled at load balancer

### Q7: What is session persistence and when do you need it?

**Answer:**
- **Session Persistence**: Route same client to same server
- **Stateful Applications**: Applications with server-side state
- **Session Data**: Session data stored on server
- **Methods**: Cookie-based, IP hash, application-level
- **Trade-off**: May cause load imbalance

### Q8: How do you handle session persistence in a stateless architecture?

**Answer:**
- **Shared Session Store**: Use Redis/Memcached for sessions
- **Stateless Servers**: Servers don't store session data
- **No Sticky Sessions**: No need for sticky sessions
- **Scalability**: Better scalability
- **Flexibility**: Any server can handle any request

### Q9: What are the benefits of using a load balancer?

**Answer:**
1. **High Availability**: Service availability
2. **Scalability**: Horizontal scaling
3. **Performance**: Better performance
4. **Flexibility**: Add/remove servers easily
5. **Security**: DDoS protection, SSL termination

### Q10: What are the challenges of using load balancers?

**Answer:**
1. **Complexity**: Additional complexity
2. **Single Point of Failure**: Load balancer can fail
3. **Latency**: Adds network latency
4. **Configuration**: Complex configuration
5. **Cost**: Additional infrastructure cost

---

## Summary

### Key Takeaways

1. **Load Balancer Purpose**: Distribute traffic, ensure availability, improve performance
2. **Types**: Layer 4 (fast, simple) vs Layer 7 (flexible, feature-rich)
3. **Algorithms**: Round robin, least connections, IP hash, weighted
4. **Health Checks**: Critical for removing unhealthy servers
5. **High Availability**: Active-passive or active-active setup
6. **SSL Termination**: Handle SSL at load balancer for performance
7. **Session Persistence**: Cookie-based or IP hash for stateful apps
8. **Best Practices**: Health checks, timeouts, monitoring, security

### Common Patterns

- **Web Apps**: Load balancer → Web servers → App servers
- **APIs**: API Gateway → Load balancer → API servers
- **Microservices**: Service mesh → Load balancer → Services
- **Databases**: Load balancer → Master (write) / Replicas (read)

---

**Guide Complete** - Comprehensive guide to load balancer basics, usage, and configuration!

