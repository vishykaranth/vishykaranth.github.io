# NoSQL Locking, Case Studies, Troubleshooting, and Decision Guide

## Table of Contents
1. [NoSQL Database Examples](#nosql-database-examples)
2. [Additional Case Studies](#additional-case-studies)
3. [Troubleshooting Guide](#troubleshooting-guide)
4. [Decision Flowchart and Guide](#decision-flowchart-and-guide)

---

## NoSQL Database Examples

### MongoDB

#### Pessimistic Locking in MongoDB

MongoDB doesn't have traditional row-level locks like SQL databases, but you can achieve similar behavior using:

1. **FindOneAndUpdate with Atomic Operations**
2. **Transactions (MongoDB 4.0+)**
3. **Optimistic Concurrency Control**

#### Atomic Operations (Pessimistic-like)

```javascript
// MongoDB atomic update (prevents concurrent modifications)
function transferMoneyMongoDB(fromAccountId, toAccountId, amount) {
    const session = client.startSession();
    
    try {
        session.startTransaction();
        
        // Atomic find and update (acts like pessimistic lock)
        const fromAccount = await db.collection('accounts').findOneAndUpdate(
            { 
                account_id: fromAccountId,
                balance: { $gte: amount }  // Ensure sufficient balance
            },
            { 
                $inc: { balance: -amount },
                $set: { last_updated: new Date() }
            },
            { 
                session: session,
                returnDocument: 'after'  // Return updated document
            }
        );
        
        if (!fromAccount) {
            throw new Error('Insufficient funds or account not found');
        }
        
        // Atomic update for recipient
        const toAccount = await db.collection('accounts').findOneAndUpdate(
            { account_id: toAccountId },
            { 
                $inc: { balance: amount },
                $set: { last_updated: new Date() }
            },
            { 
                session: session,
                returnDocument: 'after'
            }
        );
        
        if (!toAccount) {
            throw new Error('Recipient account not found');
        }
        
        await session.commitTransaction();
        
        return { fromAccount, toAccount };
        
    } catch (error) {
        await session.abortTransaction();
        throw error;
    } finally {
        session.endSession();
    }
}
```

#### MongoDB Transactions (Multi-Document)

```javascript
// Multi-document transaction (MongoDB 4.0+)
async function bookSeatMongoDB(eventId, seatId, userId) {
    const session = client.startSession();
    
    try {
        session.startTransaction();
        
        // Find and update seat atomically
        const seat = await db.collection('seats').findOneAndUpdate(
            { 
                event_id: eventId,
                seat_id: seatId,
                status: 'available'
            },
            { 
                $set: { 
                    status: 'booked',
                    user_id: userId,
                    booked_at: new Date()
                }
            },
            { 
                session: session,
                returnDocument: 'after'
            }
        );
        
        if (!seat) {
            throw new Error('Seat not available');
        }
        
        // Create booking document
        const booking = await db.collection('bookings').insertOne(
            {
                user_id: userId,
                event_id: eventId,
                seat_id: seatId,
                status: 'confirmed',
                created_at: new Date()
            },
            { session: session }
        );
        
        await session.commitTransaction();
        return booking.insertedId;
        
    } catch (error) {
        await session.abortTransaction();
        throw error;
    } finally {
        session.endSession();
    }
}
```

#### Optimistic Locking in MongoDB

```javascript
// Optimistic locking with version field
async function updateProductStockOptimistic(productId, quantity, maxRetries = 3) {
    for (let attempt = 0; attempt < maxRetries; attempt++) {
        try {
            // Read current document
            const product = await db.collection('products').findOne({
                product_id: productId
            });
            
            if (!product) {
                throw new Error('Product not found');
            }
            
            if (product.stock < quantity) {
                throw new Error('Insufficient stock');
            }
            
            const currentVersion = product.version;
            
            // Update with version check
            const result = await db.collection('products').updateOne(
                {
                    product_id: productId,
                    version: currentVersion,  // Version check
                    stock: { $gte: quantity }  // Additional check
                },
                {
                    $inc: { 
                        stock: -quantity,
                        version: 1  // Increment version
                    },
                    $set: { last_updated: new Date() }
                }
            );
            
            if (result.matchedCount === 0) {
                // Conflict detected - version changed or stock insufficient
                if (attempt === maxRetries - 1) {
                    throw new Error('Concurrent modification or insufficient stock');
                }
                // Exponential backoff
                await new Promise(resolve => setTimeout(resolve, 100 * Math.pow(2, attempt)));
                continue;
            }
            
            // Success
            return result;
            
        } catch (error) {
            if (attempt === maxRetries - 1) {
                throw error;
            }
            await new Promise(resolve => setTimeout(resolve, 100 * Math.pow(2, attempt)));
        }
    }
}
```

#### MongoDB Change Streams for Real-time Updates

```javascript
// Monitor changes in real-time
function watchSeatAvailability(eventId) {
    const changeStream = db.collection('seats').watch(
        [
            { $match: { 'fullDocument.event_id': eventId } }
        ],
        { fullDocument: 'updateLookup' }
    );
    
    changeStream.on('change', (change) => {
        if (change.operationType === 'update') {
            const seat = change.fullDocument;
            console.log(`Seat ${seat.seat_id} is now ${seat.status}`);
            // Notify clients of availability change
            notifyClients(seat);
        }
    });
    
    return changeStream;
}
```

#### Python Example (PyMongo)

```python
from pymongo import MongoClient
from pymongo.errors import OperationFailure
import time

def transfer_money_mongodb(from_account_id, to_account_id, amount):
    """
    Transfer money using MongoDB transactions
    """
    client = MongoClient('mongodb://localhost:27017/')
    db = client['banking']
    session = client.start_session()
    
    try:
        session.start_transaction()
        
        # Atomic update for sender
        from_result = db.accounts.find_one_and_update(
            {
                'account_id': from_account_id,
                'balance': {'$gte': amount}
            },
            {
                '$inc': {'balance': -amount},
                '$set': {'last_updated': datetime.now()}
            },
            session=session,
            return_document=True
        )
        
        if not from_result:
            raise ValueError('Insufficient funds or account not found')
        
        # Atomic update for recipient
        to_result = db.accounts.find_one_and_update(
            {'account_id': to_account_id},
            {
                '$inc': {'balance': amount},
                '$set': {'last_updated': datetime.now()}
            },
            session=session,
            return_document=True
        )
        
        if not to_result:
            raise ValueError('Recipient account not found')
        
        session.commit_transaction()
        return {'from': from_result, 'to': to_result}
        
    except Exception as e:
        session.abort_transaction()
        raise
    finally:
        session.end_session()

def update_stock_optimistic(product_id, quantity, max_retries=3):
    """
    Update stock with optimistic locking
    """
    client = MongoClient('mongodb://localhost:27017/')
    db = client['inventory']
    
    for attempt in range(max_retries):
        try:
            # Read product
            product = db.products.find_one({'product_id': product_id})
            
            if not product:
                raise ValueError('Product not found')
            
            if product['stock'] < quantity:
                raise ValueError('Insufficient stock')
            
            current_version = product.get('version', 0)
            
            # Update with version check
            result = db.products.update_one(
                {
                    'product_id': product_id,
                    'version': current_version,
                    'stock': {'$gte': quantity}
                },
                {
                    '$inc': {
                        'stock': -quantity,
                        'version': 1
                    },
                    '$set': {'last_updated': datetime.now()}
                }
            )
            
            if result.matched_count == 0:
                if attempt == max_retries - 1:
                    raise ValueError('Concurrent modification detected')
                time.sleep(0.1 * (2 ** attempt))
                continue
            
            return result
            
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(0.1 * (2 ** attempt))
```

---

### Apache Cassandra

#### Cassandra's Approach to Concurrency

Cassandra uses **Last Write Wins (LWW)** by default, but you can implement optimistic locking using:

1. **Lightweight Transactions (LWT)** - Compare-and-Set operations
2. **Timestamp-based versioning**
3. **Vector Clocks** (for distributed systems)

#### Lightweight Transactions (Pessimistic-like)

```cql
-- Lightweight Transaction (Compare-and-Set)
-- Acts like pessimistic locking

-- Update only if condition is met
UPDATE accounts
SET balance = balance - 100
WHERE account_id = 'acc-123'
IF balance >= 100;

-- Returns: [applied] | [balance] | [balance] (old value)
```

```python
from cassandra.cluster import Cluster
from cassandra.query import SimpleStatement
from cassandra import ConsistencyLevel

def transfer_money_cassandra(from_account_id, to_account_id, amount):
    """
    Transfer money using Cassandra Lightweight Transactions
    """
    cluster = Cluster(['127.0.0.1'])
    session = cluster.connect('banking')
    
    # Use QUORUM consistency for LWT
    session.default_consistency_level = ConsistencyLevel.QUORUM
    
    try:
        # Step 1: Check and deduct from sender (atomic)
        deduct_query = SimpleStatement("""
            UPDATE accounts
            SET balance = balance - ?
            WHERE account_id = ?
            IF balance >= ?
        """, consistency_level=ConsistencyLevel.QUORUM)
        
        result = session.execute(deduct_query, [amount, from_account_id, amount])
        
        if not result[0].applied:
            raise ValueError('Insufficient funds')
        
        # Step 2: Add to recipient
        add_query = SimpleStatement("""
            UPDATE accounts
            SET balance = balance + ?
            WHERE account_id = ?
            IF EXISTS
        """, consistency_level=ConsistencyLevel.QUORUM)
        
        result = session.execute(add_query, [amount, to_account_id])
        
        if not result[0].applied:
            # Rollback: refund to sender
            refund_query = SimpleStatement("""
                UPDATE accounts
                SET balance = balance + ?
                WHERE account_id = ?
            """, consistency_level=ConsistencyLevel.QUORUM)
            session.execute(refund_query, [amount, from_account_id])
            raise ValueError('Recipient account not found')
        
        return {'status': 'success'}
        
    except Exception as e:
        raise
    finally:
        cluster.shutdown()
```

#### Optimistic Locking with Timestamps

```cql
-- Table with timestamp for optimistic locking
CREATE TABLE products (
    product_id UUID PRIMARY KEY,
    name TEXT,
    stock INT,
    last_updated TIMESTAMP
);

-- Update with timestamp check
UPDATE products
SET stock = stock - 10,
    last_updated = toTimestamp(now())
WHERE product_id = ?
IF last_updated = ?;  -- Timestamp check
```

```python
def update_stock_optimistic_cassandra(product_id, quantity, expected_timestamp, max_retries=3):
    """
    Update stock with optimistic locking using timestamps
    """
    cluster = Cluster(['127.0.0.1'])
    session = cluster.connect('inventory')
    session.default_consistency_level = ConsistencyLevel.QUORUM
    
    for attempt in range(max_retries):
        try:
            # Read current state
            select_query = SimpleStatement("""
                SELECT stock, last_updated
                FROM products
                WHERE product_id = ?
            """)
            
            result = session.execute(select_query, [product_id])
            product = result.one()
            
            if not product:
                raise ValueError('Product not found')
            
            if product.stock < quantity:
                raise ValueError('Insufficient stock')
            
            # Update with timestamp check
            update_query = SimpleStatement("""
                UPDATE products
                SET stock = stock - ?,
                    last_updated = toTimestamp(now())
                WHERE product_id = ?
                IF last_updated = ?
                AND stock >= ?
            """, consistency_level=ConsistencyLevel.QUORUM)
            
            result = session.execute(
                update_query, 
                [quantity, product_id, product.last_updated, quantity]
            )
            
            if not result[0].applied:
                # Conflict detected
                if attempt == max_retries - 1:
                    raise ValueError('Concurrent modification detected')
                time.sleep(0.1 * (2 ** attempt))
                continue
            
            return {'status': 'success'}
            
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(0.1 * (2 ** attempt))
    finally:
        cluster.shutdown()
```

#### Vector Clocks for Distributed Systems

```python
from dataclasses import dataclass
from typing import Dict
import uuid

@dataclass
class VectorClock:
    """
    Vector clock for distributed optimistic locking
    """
    clocks: Dict[str, int]  # node_id -> timestamp
    
    def increment(self, node_id: str):
        """Increment clock for this node"""
        self.clocks[node_id] = self.clocks.get(node_id, 0) + 1
    
    def happens_before(self, other: 'VectorClock') -> bool:
        """Check if this happens before other"""
        return all(
            self.clocks.get(node, 0) <= other.clocks.get(node, 0)
            for node in set(self.clocks.keys()) | set(other.clocks.keys())
        ) and any(
            self.clocks.get(node, 0) < other.clocks.get(node, 0)
            for node in set(self.clocks.keys()) | set(other.clocks.keys())
        )
    
    def merge(self, other: 'VectorClock'):
        """Merge with another vector clock"""
        for node, time in other.clocks.items():
            self.clocks[node] = max(self.clocks.get(node, 0), time)

def update_with_vector_clock(product_id, quantity, node_id, max_retries=3):
    """
    Update using vector clocks for conflict detection
    """
    cluster = Cluster(['127.0.0.1'])
    session = cluster.connect('inventory')
    
    for attempt in range(max_retries):
        try:
            # Read with vector clock
            result = session.execute("""
                SELECT stock, vector_clock
                FROM products
                WHERE product_id = ?
            """, [product_id])
            
            product = result.one()
            current_clock = VectorClock(product.vector_clock)
            
            # Check for conflicts
            if current_clock.happens_before(VectorClock({node_id: 0})):
                # Conflict detected
                if attempt == max_retries - 1:
                    raise ValueError('Concurrent modification detected')
                time.sleep(0.1 * (2 ** attempt))
                continue
            
            # Increment our clock
            current_clock.increment(node_id)
            
            # Update with vector clock check
            result = session.execute("""
                UPDATE products
                SET stock = stock - ?,
                    vector_clock = ?
                WHERE product_id = ?
                IF vector_clock = ?
            """, [quantity, current_clock.clocks, product_id, product.vector_clock])
            
            if not result[0].applied:
                raise ValueError('Update failed')
            
            return {'status': 'success'}
            
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(0.1 * (2 ** attempt))
    finally:
        cluster.shutdown()
```

#### Cassandra Best Practices

```python
# 1. Use appropriate consistency levels
# QUORUM for critical operations
session.default_consistency_level = ConsistencyLevel.QUORUM

# 2. Batch operations for atomicity
batch = BatchStatement(consistency_level=ConsistencyLevel.QUORUM)
batch.add(SimpleStatement("UPDATE accounts SET balance = balance - ? WHERE account_id = ?"), [amount, from_id])
batch.add(SimpleStatement("UPDATE accounts SET balance = balance + ? WHERE account_id = ?"), [amount, to_id])
session.execute(batch)

# 3. Handle LWT failures gracefully
try:
    result = session.execute(update_query, params)
    if not result[0].applied:
        # Handle conflict
        handle_conflict()
except Exception as e:
    # Handle error
    handle_error(e)
```

---

## Additional Case Studies

### Case Study 6: Netflix's Content Recommendation System

#### Challenge
- Millions of users
- Real-time recommendation updates
- High read, low write
- Personalization per user

#### Solution: Optimistic Locking with Caching

```python
class NetflixRecommendation:
    def __init__(self):
        self.redis = Redis()  # Cache for recommendations
        self.mongodb = MongoDB()  # User preferences
        self.cassandra = Cassandra()  # Viewing history
    
    def update_user_preferences(self, user_id, preferences):
        """
        Update user preferences with optimistic locking
        """
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # Read current preferences
                user = self.mongodb.users.find_one({'user_id': user_id})
                current_version = user.get('version', 0)
                
                # Update with version check
                result = self.mongodb.users.update_one(
                    {
                        'user_id': user_id,
                        'version': current_version
                    },
                    {
                        '$set': {'preferences': preferences},
                        '$inc': {'version': 1}
                    }
                )
                
                if result.matched_count == 0:
                    if attempt == max_retries - 1:
                        raise ConcurrentUpdateError()
                    time.sleep(0.1 * (2 ** attempt))
                    continue
                
                # Invalidate cache
                self.redis.delete(f"recommendations:{user_id}")
                
                # Trigger recommendation recalculation
                self.recalculate_recommendations(user_id)
                
                return
                
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                time.sleep(0.1 * (2 ** attempt))
    
    def get_recommendations(self, user_id):
        """
        Get recommendations (read-heavy, no locking needed)
        """
        # Try cache first
        cached = self.redis.get(f"recommendations:{user_id}")
        if cached:
            return json.loads(cached)
        
        # Calculate recommendations
        recommendations = self.calculate_recommendations(user_id)
        
        # Cache for 1 hour
        self.redis.setex(
            f"recommendations:{user_id}",
            3600,
            json.dumps(recommendations)
        )
        
        return recommendations
```

**Results:**
- **Read Latency**: < 50ms (cached)
- **Update Conflicts**: < 0.1%
- **Throughput**: 1M recommendations/second
- **Personalization**: Real-time updates

---

### Case Study 7: Stripe's Payment Processing

#### Challenge
- Financial transactions (critical)
- High security requirements
- Prevent double charging
- Handle concurrent payment attempts

#### Solution: Pessimistic Locking with Idempotency

```python
class StripePayment:
    def __init__(self):
        self.db = PostgreSQL()
        self.redis = Redis()
    
    def process_payment(self, payment_id, amount, idempotency_key):
        """
        Process payment with pessimistic locking and idempotency
        """
        # Check idempotency first
        existing = self.redis.get(f"idempotency:{idempotency_key}")
        if existing:
            return json.loads(existing)
        
        with self.db.transaction():
            # Lock payment record
            payment = self.db.query("""
                SELECT payment_id, status, amount
                FROM payments
                WHERE payment_id = ?
                FOR UPDATE
            """, payment_id)
            
            if not payment:
                raise PaymentNotFoundError()
            
            if payment.status != 'pending':
                raise InvalidPaymentStatusError(f"Payment is {payment.status}")
            
            # Process with payment gateway
            try:
                charge_result = self.stripe_gateway.charge(
                    amount=amount,
                    payment_method=payment.payment_method_id
                )
                
                # Update payment status
                self.db.execute("""
                    UPDATE payments
                    SET status = 'completed',
                        transaction_id = ?,
                        processed_at = NOW()
                    WHERE payment_id = ?
                """, charge_result.transaction_id, payment_id)
                
                # Store idempotency key
                result = {
                    'payment_id': payment_id,
                    'status': 'completed',
                    'transaction_id': charge_result.transaction_id
                }
                
                self.redis.setex(
                    f"idempotency:{idempotency_key}",
                    86400,  # 24 hours
                    json.dumps(result)
                )
                
                self.db.commit()
                return result
                
            except PaymentGatewayError as e:
                # Update payment status
                self.db.execute("""
                    UPDATE payments
                    SET status = 'failed',
                        error_message = ?
                    WHERE payment_id = ?
                """, str(e), payment_id)
                
                self.db.commit()
                raise
```

**Results:**
- **Double Charging**: 0%
- **Idempotency**: 100%
- **Transaction Safety**: Guaranteed
- **Throughput**: 10K payments/second

---

### Case Study 8: Twitter's Like/Retweet System

#### Challenge
- Millions of concurrent likes/retweets
- Real-time counters
- Prevent duplicate actions
- High availability

#### Solution: Optimistic Locking with Distributed Counters

```python
class TwitterEngagement:
    def __init__(self):
        self.redis = Redis()  # Fast counters
        self.cassandra = Cassandra()  # Engagement records
        self.kafka = Kafka()  # Event streaming
    
    def like_tweet(self, user_id, tweet_id):
        """
        Like a tweet with optimistic locking
        """
        max_retries = 3
        engagement_key = f"engagement:{tweet_id}:{user_id}"
        
        for attempt in range(max_retries):
            try:
                # Check if already liked (idempotency)
                existing = self.redis.get(engagement_key)
                if existing and json.loads(existing)['action'] == 'like':
                    return {'status': 'already_liked'}
                
                # Read current engagement count
                current_count = self.redis.get(f"likes:{tweet_id}") or 0
                
                # Increment counter atomically
                new_count = self.redis.incr(f"likes:{tweet_id}")
                
                # Store engagement record
                engagement = {
                    'user_id': user_id,
                    'tweet_id': tweet_id,
                    'action': 'like',
                    'timestamp': datetime.now().isoformat()
                }
                
                # Write to Cassandra (async)
                self.cassandra.engagements.insert(engagement)
                
                # Publish event
                self.kafka.publish('tweet_engagement', engagement)
                
                # Cache engagement
                self.redis.setex(
                    engagement_key,
                    86400,  # 24 hours
                    json.dumps(engagement)
                )
                
                return {'status': 'liked', 'count': new_count}
                
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                time.sleep(0.1 * (2 ** attempt))
    
    def unlike_tweet(self, user_id, tweet_id):
        """
        Unlike a tweet
        """
        engagement_key = f"engagement:{tweet_id}:{user_id}"
        
        # Check if liked
        existing = self.redis.get(engagement_key)
        if not existing or json.loads(existing)['action'] != 'like':
            return {'status': 'not_liked'}
        
        # Decrement counter
        new_count = self.redis.decr(f"likes:{tweet_id}")
        
        # Remove engagement
        self.redis.delete(engagement_key)
        
        # Update Cassandra
        self.cassandra.engagements.delete(
            {'user_id': user_id, 'tweet_id': tweet_id, 'action': 'like'}
        )
        
        return {'status': 'unliked', 'count': new_count}
```

**Results:**
- **Throughput**: 100K likes/second
- **Latency**: < 10ms
- **Duplicate Prevention**: 100%
- **Real-time Counters**: Accurate

---

### Case Study 9: Airbnb's Booking System

#### Challenge
- Property availability management
- Prevent double booking
- Handle timezone issues
- Real-time availability updates

#### Solution: Two-Phase Booking with Date Range Locking

```python
class AirbnbBooking:
    def __init__(self):
        self.db = PostgreSQL()
        self.redis = Redis()
    
    def check_availability(self, property_id, check_in, check_out):
        """
        Check availability (read-only, no locking)
        """
        # Try cache first
        cache_key = f"availability:{property_id}:{check_in}:{check_out}"
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # Query database
        bookings = self.db.query("""
            SELECT booking_id
            FROM bookings
            WHERE property_id = ?
            AND status IN ('pending', 'confirmed')
            AND (
                (check_in <= ? AND check_out > ?) OR
                (check_in < ? AND check_out >= ?) OR
                (check_in >= ? AND check_out <= ?)
            )
        """, property_id, check_in, check_in, check_out, check_out, check_in, check_out)
        
        available = len(bookings) == 0
        
        # Cache for 5 minutes
        self.redis.setex(cache_key, 300, json.dumps({'available': available}))
        
        return {'available': available}
    
    def create_booking(self, property_id, user_id, check_in, check_out):
        """
        Create booking with pessimistic locking on date range
        """
        with self.db.transaction():
            # Lock overlapping bookings
            overlapping = self.db.query("""
                SELECT booking_id
                FROM bookings
                WHERE property_id = ?
                AND status IN ('pending', 'confirmed')
                AND (
                    (check_in <= ? AND check_out > ?) OR
                    (check_in < ? AND check_out >= ?) OR
                    (check_in >= ? AND check_out <= ?)
                )
                FOR UPDATE
            """, property_id, check_in, check_in, check_out, check_out, check_in, check_out)
            
            if overlapping:
                raise PropertyUnavailableError("Dates are already booked")
            
            # Create booking
            booking_id = self.db.execute("""
                INSERT INTO bookings (
                    property_id, user_id, check_in, check_out, status
                )
                VALUES (?, ?, ?, ?, 'pending')
                RETURNING booking_id
            """, property_id, user_id, check_in, check_out)
            
            # Invalidate cache
            self.redis.delete(f"availability:{property_id}:*")
            
            self.db.commit()
            return booking_id
    
    def confirm_booking(self, booking_id, payment_id):
        """
        Confirm booking after payment
        """
        with self.db.transaction():
            booking = self.db.query("""
                SELECT * FROM bookings
                WHERE booking_id = ?
                FOR UPDATE
            """, booking_id)
            
            if booking.status != 'pending':
                raise InvalidBookingStatusError()
            
            # Update status
            self.db.execute("""
                UPDATE bookings
                SET status = 'confirmed',
                    payment_id = ?,
                    confirmed_at = NOW()
                WHERE booking_id = ?
            """, payment_id, booking_id)
            
            # Invalidate cache
            self.redis.delete(f"availability:{booking.property_id}:*")
            
            self.db.commit()
```

**Results:**
- **Double Booking**: 0%
- **Availability Accuracy**: 99.9%
- **Booking Success Rate**: 98%
- **Cache Hit Rate**: 85%

---

### Case Study 10: Spotify's Playlist Management

#### Challenge
- Concurrent playlist edits
- Real-time collaboration
- Conflict resolution
- High read traffic

#### Solution: Operational Transformation with Optimistic Locking

```python
class SpotifyPlaylist:
    def __init__(self):
        self.db = MongoDB()
        self.redis = Redis()
    
    def add_track_to_playlist(self, playlist_id, track_id, position, user_id):
        """
        Add track with operational transformation
        """
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                # Read playlist
                playlist = self.db.playlists.find_one({'playlist_id': playlist_id})
                current_version = playlist.get('version', 0)
                
                # Transform operation based on concurrent edits
                transformed_position = self.transform_position(
                    position,
                    playlist.get('pending_operations', [])
                )
                
                # Update with version check
                result = self.db.playlists.update_one(
                    {
                        'playlist_id': playlist_id,
                        'version': current_version
                    },
                    {
                        '$push': {
                            'tracks': {
                                '$each': [{'track_id': track_id, 'added_by': user_id}],
                                '$position': transformed_position
                            }
                        },
                        '$inc': {'version': 1},
                        '$push': {
                            'operations': {
                                'type': 'add_track',
                                'track_id': track_id,
                                'position': position,
                                'user_id': user_id,
                                'timestamp': datetime.now()
                            }
                        }
                    }
                )
                
                if result.matched_count == 0:
                    if attempt == max_retries - 1:
                        raise ConcurrentEditError()
                    time.sleep(0.1 * (2 ** attempt))
                    continue
                
                # Invalidate cache
                self.redis.delete(f"playlist:{playlist_id}")
                
                # Notify collaborators
                self.notify_collaborators(playlist_id, 'track_added')
                
                return {'status': 'success'}
                
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                time.sleep(0.1 * (2 ** attempt))
    
    def transform_position(self, position, pending_operations):
        """
        Transform position based on concurrent operations
        """
        adjusted_position = position
        
        for op in pending_operations:
            if op['type'] == 'add_track' and op['position'] <= position:
                adjusted_position += 1
            elif op['type'] == 'remove_track' and op['position'] < position:
                adjusted_position -= 1
        
        return adjusted_position
```

**Results:**
- **Concurrent Edits**: Handled gracefully
- **Conflict Resolution**: Automatic
- **User Experience**: Smooth collaboration
- **Throughput**: 50K edits/second

---

## Troubleshooting Guide

### Common Issue 1: High Retry Rate in Optimistic Locking

#### Symptoms
- Many retries in optimistic locking
- Poor performance
- High CPU usage

#### Diagnosis

```python
# Add monitoring
class OptimisticLockMonitor:
    def __init__(self):
        self.retry_count = 0
        self.total_attempts = 0
        self.conflict_rate = 0
    
    def track_attempt(self, retried):
        self.total_attempts += 1
        if retried:
            self.retry_count += 1
        self.conflict_rate = self.retry_count / self.total_attempts
    
    def get_metrics(self):
        return {
            'retry_rate': self.conflict_rate,
            'total_attempts': self.total_attempts,
            'retries': self.retry_count
        }
```

#### Solutions

1. **Switch to Pessimistic Locking**
```python
# If conflict rate > 20%, use pessimistic
if conflict_rate > 0.2:
    return use_pessimistic_locking()
else:
    return use_optimistic_locking()
```

2. **Increase Retry Backoff**
```python
# Exponential backoff with jitter
wait_time = base_delay * (2 ** attempt) + random.uniform(0, jitter)
time.sleep(wait_time)
```

3. **Partition Data**
```python
# Reduce contention by partitioning
def get_partition(resource_id, num_partitions):
    return resource_id % num_partitions

# Lock only within partition
partition = get_partition(account_id, 10)
lock_key = f"lock:partition:{partition}:account:{account_id}"
```

---

### Common Issue 2: Deadlocks in Pessimistic Locking

#### Symptoms
- Transactions timing out
- Database deadlock errors
- Application errors

#### Diagnosis

```sql
-- PostgreSQL: Check for deadlocks
SELECT 
    datname,
    deadlocks,
    deadlocks / NULLIF(xact_commit + xact_rollback, 0) as deadlock_rate
FROM pg_stat_database
WHERE datname = 'mydb';

-- MySQL: Check InnoDB status
SHOW ENGINE INNODB STATUS;
```

#### Solutions

1. **Consistent Lock Ordering**
```python
def acquire_locks_safely(resources):
    """
    Always acquire locks in sorted order
    """
    sorted_resources = sorted(resources)
    locks = []
    
    for resource in sorted_resources:
        lock = acquire_lock(resource)
        locks.append(lock)
    
    return locks
```

2. **Lock Timeout**
```python
# Set timeout
SET lock_timeout = '5s';

# Or in application
def acquire_lock_with_timeout(resource, timeout=5):
    start = time.time()
    while time.time() - start < timeout:
        if try_acquire_lock(resource):
            return True
        time.sleep(0.01)
    raise LockTimeoutError()
```

3. **Minimize Lock Duration**
```python
# BAD: Long lock duration
def process_order_bad(order_id):
    with db.transaction():
        order = db.query("SELECT * FROM orders WHERE id = ? FOR UPDATE", order_id)
        # External API call (5 seconds!)
        payment_result = payment_gateway.charge(order.amount)
        db.execute("UPDATE orders SET status = 'paid' WHERE id = ?", order_id)

# GOOD: Minimize lock duration
def process_order_good(order_id):
    order = db.query("SELECT * FROM orders WHERE id = ?", order_id)  # No lock
    payment_result = payment_gateway.charge(order.amount)  # No lock held
    with db.transaction():
        db.execute("""
            UPDATE orders SET status = 'paid' 
            WHERE id = ? AND status = 'pending'
        """, order_id)  # Lock only when updating
```

---

### Common Issue 3: Stale Data in Optimistic Locking

#### Symptoms
- Updates failing unexpectedly
- Version mismatches
- Data inconsistency

#### Diagnosis

```python
# Track version mismatches
class VersionMismatchMonitor:
    def __init__(self):
        self.mismatches = 0
        self.total_updates = 0
    
    def track_update(self, success):
        self.total_updates += 1
        if not success:
            self.mismatches += 1
    
    def get_mismatch_rate(self):
        return self.mismatches / self.total_updates if self.total_updates > 0 else 0
```

#### Solutions

1. **Refresh Data After Conflict**
```python
def update_with_refresh(resource_id, update_data, max_retries=3):
    for attempt in range(max_retries):
        try:
            # Read fresh data
            resource = read_resource(resource_id)
            
            # Apply update
            result = update_resource(resource_id, resource.version, update_data)
            
            if result.success:
                return result
            
            # Conflict - refresh and retry
            if attempt < max_retries - 1:
                time.sleep(0.1 * (2 ** attempt))
                continue
                
        except ConflictError:
            if attempt == max_retries - 1:
                raise
            time.sleep(0.1 * (2 ** attempt))
```

2. **Use Timestamps Instead of Versions**
```python
# Timestamps are more reliable
def update_with_timestamp(resource_id, update_data):
    resource = read_resource(resource_id)
    original_timestamp = resource.updated_at
    
    result = update_resource(
        resource_id,
        update_data,
        expected_timestamp=original_timestamp
    )
    
    if not result.success:
        # Re-read to get latest timestamp
        resource = read_resource(resource_id)
        raise StaleDataError(f"Data was updated at {resource.updated_at}")
```

---

### Common Issue 4: Lock Timeout Errors

#### Symptoms
- Frequent timeout errors
- Slow transactions
- User complaints

#### Diagnosis

```python
# Monitor lock wait times
class LockWaitMonitor:
    def track_lock_wait(self, resource, wait_time):
        if wait_time > 1.0:  # More than 1 second
            logger.warning(f"Long lock wait: {resource}, {wait_time}s")
            metrics.increment('lock.wait.long', tags={'resource': resource})
```

#### Solutions

1. **Optimize Queries**
```sql
-- Add indexes to speed up lock acquisition
CREATE INDEX idx_accounts_account_id ON accounts(account_id);
CREATE INDEX idx_seats_event_seat ON seats(event_id, seat_id);
```

2. **Reduce Lock Scope**
```python
# Lock only necessary rows
def update_accounts(account_ids, amount):
    # Lock only specific accounts, not entire table
    with db.transaction():
        accounts = db.query("""
            SELECT * FROM accounts
            WHERE account_id IN (?)
            FOR UPDATE
        """, account_ids)
        # ... update ...
```

3. **Use NOWAIT or SKIP LOCKED**
```python
# Fail fast instead of waiting
def try_lock_nowait(resource_id):
    try:
        resource = db.query("""
            SELECT * FROM resources
            WHERE id = ?
            FOR UPDATE NOWAIT
        """, resource_id)
        return resource
    except LockNotAvailableError:
        raise ResourceLockedError("Resource is currently locked")
```

---

### Common Issue 5: Performance Degradation Under Load

#### Symptoms
- Slow response times
- High database CPU
- Connection pool exhaustion

#### Diagnosis

```python
# Monitor performance metrics
class PerformanceMonitor:
    def track_operation(self, operation, duration, success):
        metrics.histogram('operation.duration', duration, tags={
            'operation': operation,
            'success': success
        })
        
        if duration > 1.0:  # Slow operation
            logger.warning(f"Slow operation: {operation}, {duration}s")
```

#### Solutions

1. **Add Caching**
```python
# Cache frequently read data
def get_account_cached(account_id):
    cached = redis.get(f"account:{account_id}")
    if cached:
        return json.loads(cached)
    
    account = db.query("SELECT * FROM accounts WHERE id = ?", account_id)
    redis.setex(f"account:{account_id}", 300, json.dumps(account))
    return account
```

2. **Batch Operations**
```python
# Batch multiple updates
def batch_update_accounts(updates):
    with db.transaction():
        for account_id, amount in updates:
            db.execute("""
                UPDATE accounts
                SET balance = balance + ?
                WHERE account_id = ?
            """, amount, account_id)
        db.commit()
```

3. **Read Replicas**
```python
# Use read replicas for reads
def read_account(account_id):
    # Read from replica (no locks needed)
    return read_replica.query("SELECT * FROM accounts WHERE id = ?", account_id)

def update_account(account_id, amount):
    # Write to primary (with locks)
    with primary_db.transaction():
        primary_db.execute("""
            UPDATE accounts SET balance = balance + ?
            WHERE account_id = ?
            FOR UPDATE
        """, amount, account_id)
```

---

## Decision Flowchart and Guide

### Visual Decision Flowchart

```
START: Need to Update Shared Resource?
│
├─ Is data CRITICAL? (Financial, Medical, Legal)
│  │
│  ├─ YES → Use PESSIMISTIC LOCKING
│  │        │
│  │        ├─ Can you minimize lock duration?
│  │        │  YES → Use pessimistic with short transactions
│  │        │  NO → Consider two-phase commit
│  │        │
│  │        └─ Prevent deadlocks:
│  │           - Consistent lock ordering
│  │           - Lock timeouts
│  │           - Minimize lock scope
│  │
│  └─ NO → Continue...
│
├─ What is the CONFLICT RATE?
│  │
│  ├─ HIGH (>20%) → Use PESSIMISTIC LOCKING
│  │                 - Blocks but prevents conflicts
│  │                 - Better for high contention
│  │
│  ├─ MEDIUM (5-20%) → Use HYBRID APPROACH
│  │                    - Try optimistic first
│  │                    - Fallback to pessimistic
│  │
│  └─ LOW (<5%) → Use OPTIMISTIC LOCKING
│                  - High concurrency
│                  - Better performance
│
├─ What is the READ/WRITE RATIO?
│  │
│  ├─ READ-HEAVY (>10:1) → Use OPTIMISTIC LOCKING
│  │                        - Many concurrent reads
│  │                        - Few writes
│  │
│  ├─ BALANCED (1:1 to 10:1) → Use HYBRID
│  │
│  └─ WRITE-HEAVY (<1:1) → Use PESSIMISTIC LOCKING
│                           - Serialize writes
│                           - Prevent conflicts
│
├─ What is the TRANSACTION DURATION?
│  │
│  ├─ SHORT (<100ms) → Use PESSIMISTIC LOCKING
│  │                    - Quick lock/release
│  │                    - Minimal blocking
│  │
│  ├─ MEDIUM (100ms-1s) → Use HYBRID
│  │
│  └─ LONG (>1s) → Use OPTIMISTIC LOCKING
│                   - Don't hold locks long
│                   - Better user experience
│
├─ Is the system DISTRIBUTED?
│  │
│  ├─ YES → Use OPTIMISTIC LOCKING or DISTRIBUTED LOCKS
│  │        - Database locks don't work across nodes
│  │        - Use Redis/Zookeeper for distributed locks
│  │        - Or use optimistic with versioning
│  │
│  └─ NO → Continue with database locks
│
└─ FINAL DECISION
   │
   ├─ PESSIMISTIC: Critical data, high conflict, short transactions
   ├─ OPTIMISTIC: Low conflict, read-heavy, long transactions
   └─ HYBRID: Medium conflict, balanced workload
```

### Decision Matrix

| Scenario | Conflict Rate | Read/Write Ratio | Transaction Duration | Recommended Approach |
|----------|---------------|------------------|----------------------|---------------------|
| Banking Transfer | High | 1:1 | Short | **Pessimistic** |
| E-commerce Inventory | Low | 10:1 | Short | **Optimistic** |
| Ticket Booking | High | 5:1 | Medium | **Two-Phase (Hybrid)** |
| Social Media Likes | Low | 100:1 | Short | **Optimistic** |
| Medical Records | Critical | 1:1 | Medium | **Pessimistic** |
| Content Management | Low | 20:1 | Long | **Optimistic** |
| Flash Sale | Very High | 1:10 | Short | **Queue + Pessimistic** |
| Collaborative Editing | Medium | 5:1 | Long | **Optimistic + OT** |

### Decision Guide Questions

Answer these questions to choose the right approach:

#### 1. Data Criticality
- **Q**: Can you tolerate any data inconsistency?
  - **NO** → Pessimistic
  - **YES** → Continue

#### 2. Conflict Rate
- **Q**: What percentage of operations conflict?
  - **>20%** → Pessimistic
  - **5-20%** → Hybrid
  - **<5%** → Optimistic

#### 3. Read/Write Pattern
- **Q**: What is the read to write ratio?
  - **>10:1 (Read-heavy)** → Optimistic
  - **<1:1 (Write-heavy)** → Pessimistic
  - **1:1 to 10:1** → Hybrid

#### 4. Transaction Duration
- **Q**: How long do transactions take?
  - **<100ms** → Pessimistic (quick lock/release)
  - **>1s** → Optimistic (don't hold locks)
  - **100ms-1s** → Hybrid

#### 5. System Architecture
- **Q**: Is the system distributed?
  - **YES** → Optimistic or Distributed Locks
  - **NO** → Database locks work

#### 6. User Experience
- **Q**: Can users wait for locks?
  - **NO** → Optimistic (non-blocking)
  - **YES** → Pessimistic (guaranteed)

### Implementation Checklist

#### For Pessimistic Locking:
- [ ] Implement consistent lock ordering
- [ ] Set lock timeouts
- [ ] Minimize lock duration
- [ ] Add deadlock detection
- [ ] Monitor lock wait times
- [ ] Use appropriate isolation levels

#### For Optimistic Locking:
- [ ] Add version/timestamp fields
- [ ] Implement retry logic
- [ ] Add exponential backoff
- [ ] Set maximum retry limit
- [ ] Monitor conflict rate
- [ ] Handle stale data gracefully

#### For Hybrid Approach:
- [ ] Implement both strategies
- [ ] Add conflict rate monitoring
- [ ] Create fallback mechanism
- [ ] Optimize based on metrics
- [ ] Test both code paths

### Performance Tuning Guide

```python
class LockingStrategySelector:
    def __init__(self):
        self.metrics = {
            'conflict_rate': 0,
            'avg_latency': 0,
            'throughput': 0
        }
    
    def select_strategy(self, operation_type, resource_type):
        """
        Dynamically select locking strategy based on metrics
        """
        # Update metrics
        self.update_metrics()
        
        # Decision logic
        if self.metrics['conflict_rate'] > 0.2:
            return 'pessimistic'
        elif self.metrics['conflict_rate'] < 0.05:
            return 'optimistic'
        else:
            return 'hybrid'
    
    def update_metrics(self):
        """
        Update metrics from monitoring system
        """
        self.metrics = {
            'conflict_rate': get_conflict_rate(),
            'avg_latency': get_avg_latency(),
            'throughput': get_throughput()
        }
```

---

## Summary

This guide provides:

1. **NoSQL Examples**: MongoDB and Cassandra implementations
2. **Case Studies**: Netflix, Stripe, Twitter, Airbnb, Spotify
3. **Troubleshooting**: Common issues and solutions
4. **Decision Guide**: Flowchart and decision matrix

Use this as a reference when designing concurrent systems!

